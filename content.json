{"pages":[{"title":"About me","text":"about siteHi, I am a full stack engineer, now living in Shanghai, recently working in the field of bigdata, often using Hadoop, Java, JS. Blogging is a very interesting thing. I write a blog to record my life feelings, technical logs and some ideas. interest sitewebsite：v2ex oschina dribbble feedly sspai dgtle readhub interest filmSuspense and science fiction movies.like: 《Source Code》 《The Butterfly Effect》","link":"/about.html"},{"title":"JFinal增加简单权限认证功能","text":"JFinal 是基于 Java 语言的极速 WEB + ORM 框架，其核心设计目标是开发迅速、代码量少、学习简单、功能强大、轻量级、易扩展、Restful。在拥有Java语言所有优势的同时再拥有ruby、python、php等动态语言的开发效率！ 基于JFinal有shiro权限认证模块。这里使用interceptor和session实现简单的权限认证功能。 Interceptor每个action请求，都会校验session中是否登录成功。 12345678910111213141516171819202122232425262728293031323334public class CommonInterceptor implements Interceptor { public static List&lt;String&gt; noLoginAction; public CommonInterceptor() { noLoginAction = new ArrayList&lt;String&gt;(); noLoginAction.add(&quot;/login&quot;); noLoginAction.add(&quot;/doLogin&quot;); noLoginAction.add(&quot;/logout&quot;); } public void intercept(Invocation inv) { Controller controller = inv.getController(); try { HttpSession session = controller.getSession(); boolean flag = (boolean) (session.getAttribute(&quot;islogin&quot;) != null ? session.getAttribute(&quot;islogin&quot;): false); if (flag) { // 登录后调整到首页，不进入login页面 if (IndexController.LOGIN_URL.equals(inv.getActionKey())) { controller.redirect(IndexController.LOGIN_SUCCESS_URL); } else { inv.invoke(); } } else if (noLoginAction.contains(inv.getActionKey())) { inv.invoke(); } else { controller.redirect(IndexController.LOGIN_URL); } } catch (Exception e) { e.printStackTrace(); controller.redirect(&quot;/&quot;); } }} Controller登录时判断用户和密码是否匹配 123456789101112131415public void login() { this.render(&quot;/auth/login.html&quot;);}public void doLogin() { String username = this.getPara(&quot;username&quot;); String password = this.getPara(&quot;password&quot;); if (&quot;admin&quot;.equals(username) &amp;&amp; password.equals(&quot;admin&quot;)) { setSessionAttr(&quot;islogin&quot;, true); } this.redirect(&quot;/&quot;);}public void logout() { setSessionAttr(&quot;islogin&quot;, false); this.redirect(LOGIN_URL);} Config在Config中增加Interceptor 123456/*** 配置全局拦截器*/public void configInterceptor(Interceptors me) { me.add(new CommonInterceptor());}","link":"/jfinal-auth.html"},{"title":"Linux下搭建SVN+Apache","text":"SVN是Subversion的简称，是一个开放源代码的版本控制系统。本文主要讲解Linux下SVN服务的搭建，同时配合Apache，可以在浏览器中访问SVN。 SVN/Apache安装 ##可以在官网下载SVN服务端安装包，官网下载：http://subversion.apache.org/packages.htmlSVN客户端：：TortoiseSVN，官网下载：http://tortoisesvn.net/downloads.html 如果有repo源，可以直接安装 123yum install httpdyum install mod_dav_svnyum install subversion 仓库创建 ##12345mkdir /var/www/svncd /var/www/svnsvnadmin create repo1ls /var/www/svn/repo1conf db format hooks locks README.txt 仓库目录说明 - hooks目录：放置hook脚本文件的目录- locks目录：用来放置subversion的db锁文件和db_logs锁文件的目录，用来追踪存取文件库的客户端- format文件：是一个文本文件，里面只放了一个整数，表示当前文件库配置的版本号- conf目录：是这个仓库的配置文件（仓库的用户访问账号、权限等） SVN配置 ##配置svn服务 ###配置文件svnserver.conf文件 1234567# vi /svn/project/conf/svnserve.conf [general] anon-access = none auth-access = write password-db = /svn/project/conf/passwd authz-db = /svn/project/conf/authz realm = My Test Repository #这是个提示信息 添加两个访问用户及口令 ###1234# vi /svn/project/conf/passwd [users] test1 = 123456 test2 = 123456 注意：对用户配置文件的修改立即生效，不必重启svn服务。 配置新用户的授权文件 ###12345678# vi /svn/project/conf/authz [groups] admin = xiaoran.shen,test1 user = test2 [/] @admin = rw @user = r * = 启动svn服务 ###12svnserve -d -r /svn/project/svnserve -d -r /var/www/svn/repo1/ 注意：不要使用系统提供的 /etc/init.d/svnserve start 来启动，因为系统默认的启动脚本中没有使用 –r /svn/project参数指定一个资源。这种情况下启动的svn服务，客户端连接会提示“svn: No repository found in ‘svn://192.168.11.229/project’ ”这样的错误。 默认svn服务器端口是3690。 Apache配置 ##授权文件和密码文件配置 ###拷贝 /var/www/svn/repo1/conf 下的 authz到 /var/www/svn下,并创建passwd文件 123cp /var/www/svn/repo1/conf/authz /var/www/svn/authz //这个是授权文件cd /var/www/svntouch passwd //生成Apache密码文件 1htpasswd /var/www/svn/passwd user1 输入前面SVN配置的密码，生成账号 将svn关联到apache上: ###1 # vi /etc/httpd/conf.d/subversion.conf 在底部加上: 12345678910111213141516&lt;Location /repos&gt; DAV svn SVNParentPath /var/www/svn # # Limit write permission to list of valid users.# &lt;LimitExcept GET PROPFIND OPTIONS REPORT&gt;# # Require SSL connection for password protection.# # SSLRequireSSL# AuthType Basic AuthName &quot;Authorization SVN&quot; AuthzSVNAccessFile /var/www/svn/authz AuthUserFile /var/www/svn/passwd Require valid-user# &lt;/LimitExcept&gt;&lt;/Location&gt; 重启httpd 1service httpd restart","link":"/backup_posts/Linux%E4%B8%8B%E6%90%AD%E5%BB%BASVN-Apache.html"},{"title":"Linux配置supervisor管理进程","text":"Supervisor是由python语言编写，基于linux操作系统的一款服务器管理工具，用以监控服务器的运行，发现问题能立即自动预警及自动重启等功能。 基础环境 Centos 6.5 Python 2.6.6 软件安装 supervisor meld(python html模版引擎) 下载地址： meld3 http://www.plope.com/software/meld3/meld3-0.6.5.tar.gz supervisor http://pypi.python.org/packages/source/s/supervisor/supervisor-3.0b1.tar.gz 下载解压，分别执行python setup.py install安装 文件配置 创建配置文件 1echo_supervisord_conf &gt; /etc/supervisord.conf 修改配置文件 12345678910111213141516171819# 增加web监控服务[inet_http_server] ; inet (TCP) server disabled by defaultport=0.0.0.0:9001 ; (ip_address:port specifier, *:port for all iface)username=user ; (default is no username (open server))password=123 ; (default is no password (open server))# 增加监控程序[program:tail1] command=tail -f /etc/supervisord.conf ;常驻后台的命令autostart=true ;是否随supervisor启动autorestart=true ;是否在挂了之后重启，意外关闭后会重启，比如kill掉！startretries=3 ;启动尝试次数stderr_logfile=/tmp/tail1.err.log ;标准输出的位置stdout_logfile=/tmp/tail1.out.log ;标准错误输出的位置# 也可以监控目录下配置文件，监控/etc/supervisord.d/目录下conf后缀文件，conf中配置监控程序[include]files=/etc/supervisord.d/*.conf 启动监控1/usr/bin/supervisord -c /etc/supervisord.conf 访问浏览器ip:9001 supervisorctl管理程序进程1234567891011121314151617181920212223# 查询状态upervisorctl status # 开启服务supervisorctl start hello # 关闭服务supervisorctl stop hello# 示例# 停止某一个进程(programxxx)，programxxx为[program:chatdemon]里配置的值，这个示例就是programnamesupervisorctl stop programname# 启动某个进程supervisorctl start programname# 重启某个进程supervisorctl restart programname# 重启所有属于名为groupworker这个分组的进程(start,restart同理)supervisorctl stop groupworker# 停止全部进程，注：start、restart、stop都不会载入最新的配置文件supervisorctl stop all# 载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程supervisorctl reload根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启supervisorctl update， 增加程序启动停止脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#!/bin/sh## /etc/init.d/supervisord## Supervisor is a client/server system that# allows its users to monitor and control a# number of processes on UNIX-like operating# systems.## chkconfig: - 64 36# description: Supervisor Server# processname: supervisord # Source init functions. /etc/rc.d/init.d/functions prog=&quot;supervisord&quot; prefix=&quot;/usr&quot;exec_prefix=&quot;${prefix}&quot;prog_bin=&quot;${exec_prefix}/bin/supervisord&quot;PIDFILE=&quot;/var/run/$prog.pid&quot; start(){ echo -n $&quot;Starting $prog: &quot; ###注意下面这一行一定得有-c /etc/supervisord.conf 不然修改了配置文件根本不生效！ daemon $prog_bin -c /etc/supervisord.conf --pidfile $PIDFILE [ -f $PIDFILE ] &amp;&amp; success $&quot;$prog startup&quot; || failure $&quot;$prog startup&quot; echo} stop(){ echo -n $&quot;Shutting down $prog: &quot; [ -f $PIDFILE ] &amp;&amp; killproc $prog || success $&quot;$prog shutdown&quot; echo} case &quot;$1&quot; in start) start ;; stop) stop ;; status) status $prog ;; restart) stop start ;; *) echo &quot;Usage: $0 {start|stop|restart|status}&quot; ;; esac 然后加入启动项呗 1234chmod +x /etc/init.d/supervisordchkconfig --add supervisordchkconfig supervisord onservice supervisord start 这样可以通过/etc/init.d/supervisord start | stop |restart 来管理supervisord 参考内容：http://supervisord.org/ 概述Flume为数据采集工具，但缺乏监控工具，Flume进程的状态不能实时看到，且进程停止不回自动重启，可以利用Supervisor来完成这些工作。 安装配置Supervisor参考文章 http://infullstack.com/linux_supervisor.html 配置/etc/supervisord.conf12345678910111213[program:flume] command=sh /usr/lib/flume/bin/flume-ng agent --conf /usr/lib/flume/conf/ -f /usr/lib/flume/conf/flume.conf -n agent1 -Dflume.root.logger=INFO,consoleautostart=true ;是否随supervisor启动autorestart=true ;是否在挂了之后重启，意外关闭后会重启，比如kill掉！startretries=3 ;启动尝试次数stderr_logfile=/tmp/flume.err.log ;标准输出的位置stdout_logfile=/tmp/flume.out.log ;标准错误输出的位置# 也可以监控目录下配置文件，监控/etc/supervisord.d/目录下conf后缀文件，conf文件中配置监控上述内容[include]files=/etc/supervisord.d/*.conf 备注程序不需要提前启动，supervisor启动的时候会启动Flume","link":"/linux-supervisor.html"},{"title":"MapReduce运行机制","text":"从逻辑实体的角度讲解mapreduce运行机制，这些按照时间顺序包括：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。 输入分片（input split）：在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切，假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），换句话说我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。 map阶段：就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行； combiner阶段：combiner阶段是程序员可以选择的，combiner其实也是一种reduce操作，因此我们看见WordCount类里是用reduce进行加载的。Combiner是一个本地化的reduce操作，它是map运算的后续操作，主要是在map计算出中间文件前做一个简单的合并重复key值的操作，例如我们对文件里的单词频率做统计，map计算时候如果碰到一个hadoop的单词就会记录为1，但是这篇文章里hadoop可能会出现n多次，那么map输出文件冗余就会很多，因此在reduce计算前对相同的key做一个合并操作，那么文件会变小，这样就提高了宽带的传输效率，毕竟hadoop计算力宽带资源往往是计算的瓶颈也是最为宝贵的资源，但是combiner操作是有风险的，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。 shuffle阶段：将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。这里我不讲怎么优化shuffle阶段，讲讲shuffle阶段的原理，因为大部分的书籍里都没讲清楚shuffle阶段。Shuffle一开始就是map阶段做输出操作，一般mapreduce计算的都是海量数据，map输出时候不可能把所有文件都放到内存操作，因此map写入磁盘的过程十分的复杂，更何况map输出时候要对结果进行排序，内存开销是很大的，map在做输出时候会在内存里开启一个环形内存缓冲区，这个缓冲区专门用来输出的，默认大小是100mb，并且在配置文件里为这个缓冲区设定了一个阀值，默认是0.80（这个大小和阀值都是可以在配置文件里进行配置的），同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阀值的80%时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill，另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作是互不干扰的，如果缓存区被撑满了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作，前面我讲到写入磁盘前会有个排序操作，这个是在写入磁盘操作时候进行，不是在写入内存时候进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。 每次spill操作也就是写入磁盘操作时候就会写一个溢出文件，也就是说在做map输出有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。这个过程里还会有一个Partitioner操作，对于这个操作很多人都很迷糊，其实Partitioner操作和map阶段的输入分片（Input split）很像，一个Partitioner对应一个reduce作业，如果我们mapreduce操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片，这个程序员可以编程控制，主要是根据实际key和value的值，根据实际业务类型或者为了更好的reduce负载均衡要求进行，这是提高reduce效率的一个关键所在。到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，程序员也可以在配置文件更改复制线程的个数，这个复制过程和map写入磁盘过程类似，也有阀值和内存大小，阀值一样可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。 reduce阶段：和map函数一样也是程序员编写的，最终结果是存储在hdfs上的。","link":"/mapreduce.html"},{"title":"MySQL和Hive的GroupBy排序","text":"## 需求 ## 分组group by的时候，需要在group by 每组的结果进行排序 如这样的需求： 需要找出所有文章中，每个作者按时间排序的文章(最新文章)，想到的语句有： 1select * from contents group by author order by pulishtime desc; 这个语句的执行结果如何？ 请先来看下where,group by,having,order by的执行顺序 group by,order by的执行顺序当一个语句中同时使用where,group by,having,order by的时候，执行顺序如下： 执行where xx对全表数据做筛选，返回第1个结果集 针对第1个结果集使用group by分组，返回第2个结果集 针对第2个结果集中的每1组数据执行select xx，有几组就执行几次，返回第3个结果集 针对第3个结集执行having xx进行筛选，返回第4个结果集 针对第4个结果集order by排序 结果由此得知，group by会在order by之前执行，所以 1select * from contents group by author order by pulishtime desc; 得到的结果，并没有按照作者、发布时间排序 Group By排序那如何实现group by排序呢？最简单的方式是，使用子查询 1234select *from (select * from contents order by pulishtime desc ) as a group by author; 这样就实现了group by排序,达到了想要的结果。","link":"/groupby.html"},{"title":"Nginx+Tomcat实现应用负载均衡","text":"当单个应用服务器压力过大时，可以使用多个应用服务器提供服务减轻单服务器的压力，再结合一个代理转发服务器，完成tomcat的负载均衡。 部署架构图 特点：当请求达到nginx，nginx会根据weight和配置的服务器列表，完成请求的转发，实现应用服务器的负载均衡。该架构适用于无状态共享的应用服务，因为session并没有设置共享。 用到工具 nginx 1.10 tomcat 1.8 环境准备三个节点 172.16.2.41 Tomcat tomcat_41 172.16.2.42 Tomcat tomcat_42 172.16.2.43 Nginx nginx_43 配置tomcat配置端口(conf/server.xml)123&lt;Connector port=&quot;1010&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; 新增首页新增应用服务器首页，tomcat/webapps/ROOT/index.html 1234[root@bigdata01 ROOT]# echo 'tomcat at 41 !' &gt; index.html[root@bigdata01 ROOT]# cat index.html tomcat at 41 ![root@bigdata01 ROOT]# 启动tomcat12345678[root@transwarp1 bin]# ./startup.sh Using CATALINA_BASE: /mnt/disk1/ray/tomcatUsing CATALINA_HOME: /mnt/disk1/ray/tomcatUsing CATALINA_TMPDIR: /mnt/disk1/ray/tomcat/tempUsing JRE_HOME: /usr/java/latestUsing CLASSPATH: /mnt/disk1/ray/tomcat/bin/bootstrap.jar:/mnt/disk1/ray/tomcat/bin/tomcat-juli.jarTomcat started.[root@transwarp1 bin]# 配置tomcat2方法同上。 部署NginxNginx 安装参考文章：Nginx 安装配置 配置nginx1234567891011121314http { upstream infullstack.com{ #集群名称 server 172.16.2.41:1010 weight=1;# 服务器列表，请求会转发到这些服务器 server 172.16.2.42:1010 weight=2; } server { listen 88; # nginx端口 server_name localhost; location / { proxy_pass http://infullstack.com; #集群名称 proxy_redirect default; } ...} 启动和访问Nginx访问http://172.16.2.43:88/ 刷新页面，会发现请求会转发到41和42服务器 根据应用服务器的配置和需求，可以修改weight，配置服务器的权重。","link":"/nginx-tomcat-load-balancing.html"},{"title":"Linux磁盘批量分区和挂载","text":"Linux磁盘批量分区和挂载 Format #!/bin/sh #DEVICE_LIST=\"\" DEVICE_LIST=\"/dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh\" for DEVICE in $DEVICE_LIST do echo \"+++++create partition for $DEVICE......\" parted -s $DEVICE mklabel gpt mkpart gpt2t ext2 0% 100% PARTITION=\"$DEVICE\"\"1\" echo \"+++++formatting $PARTITION......\" mkfs.ext4 -T largefile $PARTITION done Mount #!/bin/sh #backup /etc/fstab cp /etc/fstab /etc/fstab.bak PARTITION_LIST=\"sdb1 sdd1 sde1 sdf1 sdg1 sdh1\" for PARTITION in $PARTITION_LIST do UUID=`blkid \"/dev/\"\"$PARTITION\" | awk '{print $2}' | sed 's/\\\"//g'` echo $UUID echo \"add $PARTITION to /etc/fstab\" MOUNTDIR=\"/mnt/\"\"$PARTITION\" echo \"mkdir -p $MOUNTDIR\" mkdir -p $MOUNTDIR echo \"appending \\\"$UUID $MOUNTDIR ext4 defaults 0 0\\\" to /etc/fstab \" echo \"$UUID $MOUNTDIR ext4 defaults 0 0\" &gt;&gt; /etc/fstab echo \"\" done #mount all partitions mount -a #show mounted partitions df -h","link":"/linux-disk.html"},{"title":"Hello Hexo","text":"已经很长时间没有管理博客了，在2018快要到来之前，决心整理下了，在github上部署了Hexo，选取了以前各个博客系统的主要文章，同步到了hexo，以前的文章用markdown格式在文章迁移上，优势就体现出来了。只不过评论就不同步了。 纯静态化、可以快速部署在github或coding、高效、易于扩展，最重要的是轻量简洁，同时还不需要管理服务器，不用再担心服务器故障，在有文章发布时，push到github即可。 Hexo真是一个非常美好的博客系统。 我的Hexo配置theme: cactus-darkcomment: valine 部署hexo和域名绑定过程参考：Mac上搭建基于Github的Hexo博客 new begin以前零碎搭了多个域名和博客，这次统一入口，算一个新的开始吧。 enjoy it.","link":"/hello-hexo.html"},{"title":"instantClick让页面提前加载200ms","text":"加速网站加载的方式有很多，在@Roc的推荐下，我找到了这个InstantClick.js，仔细查看了官网的英文文档，发现InstantClick.js有个很好的实现思路(how-it-works)。 在访问者点击一个链接之前( 鼠标测试：test yourself here )： 悬停 hover (hover-&gt;click之间200ms左右) 鼠标按下 Mousedown (Mousedown-&gt;click之间100ms左右)， Touchstart 手机触碰 这两个事件之间通常有200ms的间隔，InstantClick 利用这个时间间隔预加载页面。这样当你点击页面的时候，其实页面已经加载到本地了，呈现当然也就会很快。 当然InstantClick 也使用了 Pjax： pushState 和 Ajax 技术 同时我试用了下，的确效果不错。如果你的博客需要实现Pjax，InstantClick会是个不错的选择。 使用方法下载instantclick.js下载地址：instantclick.min.js仅仅2.5Kb，很小 使用12&lt;script src=&quot;instantclick.min.js&quot; data-no-instant&gt;&lt;/script&gt;&lt;script data-no-instant&gt;InstantClick.init();&lt;/script&gt; 注： data-no-instant的含义是，这个JS只会运行一次，需要根据自己的情况，设置 如果想避免不必要的预加载，关闭hover，启用Mousedown是个不错的选择，moursedown意味着已经点击链接 查看效果打开chrome console，查看network视图，会在每次hover时，都可以先加载页面，在click时展示结果页面。 由于没有一个好的截动画软件，所以没有gif动画展示 扩展InstantClick也提供了几个事件可以设置。 change 页面更改完毕，即click触发加载后 fetch 页面开始预加载 receive 页面预加载完毕，即：hover或mousedown触发的预加载，但不一定会change，因为用户不一定click 实例因为使用ajax，所以google ga不会统计PV，所以增加change方法 12345678910&lt;script src=&quot;instantclick.min.js&quot; data-no-instant&gt;&lt;/script&gt;&lt;script data-no-instant&gt;/* Google Analytics code here, without ga('send', 'pageview') */ InstantClick.on('change', function() { ga('send', 'pageview', location.pathname + location.search);}); InstantClick.init();&lt;/script&gt;","link":"/instantclick.html"},{"title":"hive实现分页","text":"hive没有像MySQL那样的limit start,end那样的写法实现分页，所以需要使用别的方式来实现分页。 以下是我想到的两种方式： 一、借助唯一标识字段如果分页的表有唯一标识的字段，可以通过这个字段来实现分页： 获取第一页数据：注：同时需要记录这10条中最大的id为preId，作为下一页的条件。select * from table order by id asc limit 10; 获取第二页数据：注：同时保存数据中最大的id替换preId。select * from table where id &gt;preId order by id asc limit 10; 后续的页数获取同理。 二、使用row number()函数如果分页的表没有唯一标识的字段，可以通过row number()函数来实现分页。 首先使用row number()函数来给这个表做个递增的唯一标识：create table newtable as select row number(1) as id ,* from table; 通过row number函数给表加了唯一标识之后，就可以利用第一个方法来进行分页。","link":"/hive-page.html"},{"title":"java多叉树的实现","text":"概要java实现的多叉数，开发过程中偶尔需要用到 功能插入： 插入一个child节点到当前节点中 动态的插入一个新的节点到当前树中 查询： 返回当前节点的父辈节点集合 返回当前节点的晚辈集合 返回当前节点的孩子集合 找到一颗树中某个节点 遍历一棵树，层次遍历 删除： 删除节点和它下面的晚辈 删除当前节点的某个子节点 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211import java.io.Serializable;import java.util.ArrayList;import java.util.List;public class TreeNode implements Serializable { private static final long serialVersionUID = 1L; private int parentId; private int selfId; protected String nodeName; protected Object obj; protected TreeNode parentNode; protected List&lt;TreeNode&gt; childList; public TreeNode() { initChildList(); } public TreeNode(TreeNode parentNode) { this.getParentNode(); initChildList(); } public boolean isLeaf() { if (childList == null) { return true; } else { if (childList.isEmpty()) { return true; } else { return false; } } } /* 插入一个child节点到当前节点中 */ public void addChildNode(TreeNode treeNode) { initChildList(); childList.add(treeNode); } public void initChildList() { if (childList == null) childList = new ArrayList&lt;TreeNode&gt;(); } public boolean isValidTree() { return true; } /* 返回当前节点的父辈节点集合 */ public List&lt;TreeNode&gt; getElders() { List&lt;TreeNode&gt; elderList = new ArrayList&lt;TreeNode&gt;(); TreeNode parentNode = this.getParentNode(); if (parentNode == null) { return elderList; } else { elderList.add(parentNode); elderList.addAll(parentNode.getElders()); return elderList; } } /* 返回当前节点的晚辈集合 */ public List&lt;TreeNode&gt; getJuniors() { List&lt;TreeNode&gt; juniorList = new ArrayList&lt;TreeNode&gt;(); List&lt;TreeNode&gt; childList = this.getChildList(); if (childList == null) { return juniorList; } else { int childNumber = childList.size(); for (int i = 0; i &lt; childNumber; i++) { TreeNode junior = childList.get(i); juniorList.add(junior); juniorList.addAll(junior.getJuniors()); } return juniorList; } } /* 返回当前节点的孩子集合 */ public List&lt;TreeNode&gt; getChildList() { return childList; } /* 删除节点和它下面的晚辈 */ public void deleteNode() { TreeNode parentNode = this.getParentNode(); int id = this.getSelfId(); if (parentNode != null) { parentNode.deleteChildNode(id); } } /* 删除当前节点的某个子节点 */ public void deleteChildNode(int childId) { List&lt;TreeNode&gt; childList = this.getChildList(); int childNumber = childList.size(); for (int i = 0; i &lt; childNumber; i++) { TreeNode child = childList.get(i); if (child.getSelfId() == childId) { childList.remove(i); return; } } } /* 动态的插入一个新的节点到当前树中 */ public boolean insertJuniorNode(TreeNode treeNode) { int juniorParentId = treeNode.getParentId(); if (this.parentId == juniorParentId) { addChildNode(treeNode); return true; } else { List&lt;TreeNode&gt; childList = this.getChildList(); int childNumber = childList.size(); boolean insertFlag; for (int i = 0; i &lt; childNumber; i++) { TreeNode childNode = childList.get(i); insertFlag = childNode.insertJuniorNode(treeNode); if (insertFlag == true) return true; } return false; } } /* 找到一颗树中某个节点 */ public TreeNode findTreeNodeById(int id) { if (this.selfId == id) return this; if (childList.isEmpty() || childList == null) { return null; } else { int childNumber = childList.size(); for (int i = 0; i &lt; childNumber; i++) { TreeNode child = childList.get(i); TreeNode resultNode = child.findTreeNodeById(id); if (resultNode != null) { return resultNode; } } return null; } } /* 遍历一棵树，层次遍历 */ public void traverse() { if (selfId &lt; 0) return; print(this.selfId); if (childList == null || childList.isEmpty()) return; int childNumber = childList.size(); for (int i = 0; i &lt; childNumber; i++) { TreeNode child = childList.get(i); child.traverse(); } } public void print(int content) { System.out.println(String.valueOf(content)); } public void setChildList(List&lt;TreeNode&gt; childList) { this.childList = childList; } public int getParentId() { return parentId; } public void setParentId(int parentId) { this.parentId = parentId; } public int getSelfId() { return selfId; } public void setSelfId(int selfId) { this.selfId = selfId; } public TreeNode getParentNode() { return parentNode; } public void setParentNode(TreeNode parentNode) { this.parentNode = parentNode; } public String getNodeName() { return nodeName; } public void setNodeName(String nodeName) { this.nodeName = nodeName; } public Object getObj() { return obj; } public void setObj(Object obj) { this.obj = obj; }}","link":"/multifork-tree.html"},{"title":"js操作cookie","text":"通过对cookie的操作，可以利用Cookies包含信息的任意性来筛选并经常性维护这些信息，以判断在HTTP传输中的状态。 典型的应用是 判定注册用户是否已经登录网站，用户可能会得到提示，是否在下一次进入此网站时保留用户信息以便简化登录手续，这些都是Cookies的功用。 代码设置cookie 12345678//设置cookiefunction setCookie(key,value) { var now = new Date(); var end = 10; now.setTime(now.getTime() + end * 24 * 3600 * 1000); document.cookie = key + &quot;=&quot; + Q + &quot;;path=/;expires=&quot; + now.toGMTString() + &quot;;&quot;;} 获取cookie 1234567891011//获得保存在COOKIE里的key的值function getCookie(key) { var c = document.cookie.split(&quot;; &quot;); for (var i = 0; i &lt; c.length; i++) { var d = c[i].split(&quot;=&quot;); if (d[0] == key) { return unescape(d[1]); } } return '';} 使用123//比如：可以保存用户设置的页面风格，来展示不同风格setCookie(&quot;css-style&quot;,&quot;default&quot;);getCookie(&quot;css-style&quot;); 参考更丰富的参数可以参考： -JavaScript Cookies","link":"/js-cookie.html"},{"title":"kerberos定时登录设置","text":"1. 增加Crontab定时任务 12# 定时每小时20分执行一次，同时输出执行日志20 * * * * sh /user/kylin/kerberos_init.sh &gt;&gt; /user/kylin/kerberos_init.log 2&gt;&amp;1 2.脚本 kerberos_init.sh脚本 123456789## 引入Fusioninght客户端变量，CDH不需要这步source /opt/hadoop_client/bigdata_env## 引入变量source /user/root/krb5.sh## 登录kerberoskinit kylin &lt;&lt;EOFkylinEOFklist krb5.sh脚本 1234#!/bin/sh# 设置kerberos认证缓存目录export KRB5CCNAME=/user/kylin/krb5_cache 3. 环境变量(可选) 增加KRB5CCNAME变量到用户环境 12# 编辑~/.bashrc增加一行source /user/root/krb5.sh","link":"/backup_posts/kerberos%E5%AE%9A%E6%97%B6%E7%99%BB%E5%BD%95%E8%AE%BE%E7%BD%AE.html"},{"title":"kylin智能构建工具","text":"Kylin作为OLAP on Hadoop的标准，查询和构建作为Kylin的两个主要功能，可以通过Kylin的界面或者调用Kylin的RestApi、odbc和Jdbc等方式查询，构建也可以通过Kylin界面或者RestApi进行调用，但构建也会有不同类型的任务，也会遇到一些异常，如：segment overlap、构建异常或构建任务已经在执行。 本文介绍的就是调用Kylin丰富的RestApi实现一个智能的构建工具。 2. 解决问题使用Kylin进行构建时，主要会遇到下面这类问题： 频繁需要人工调用界面构建cube 自动处理构建cube中遇到的各种状况，如：已经构建、构建失败、已经有任务在执行。 构建时，还有不同的构建时间范围，如全量构建、增量构建或按指定周期构建等。 那这个时候，就需要一个能智能处理各类状况，完成不同类型构建的工具了。 3. 工具处理流程 4. 所使用到的RestApiKylin提供了全面的RestApi，可以用来查询模型，触发构建任务，执行SQL查询，获取元数据等，基于这些API，第三方系统可以与KAP紧密集成。 在智能构建工具中，需要调用到的几个不同Api有： 构建Cube-无分区 构建Cube-日期分区 删除Cube segment 查询Cube segment信息 查询构建任务状态 返回构建Job每步输出 恢复构建Job 5. 支持的功能 支持全量构建和增量构建 多种任务状态的处理 对于已经构建的Cube或segment，则刷新segment 对于正在执行的Cube或segment，则继续检测状态。 对于构建失败的Cube或segment，则恢复任务重新构建 对于已经构建了部分segment，则删除重新构建。 多种日期参数支持 月初至今 日、周、月、年 任意前后整数日期 完整的任务日志监控 6. 工具实现6.1 segment判断根据提交的构建任务的时间范围，判断当前cube中已经存在的segment的状态，以及是否有正在执行的任务 12345678910111213141516171819202122232425String str = KylinRestAPIUtil.sendGet(CommonUtils.KYLIN_HOST + &quot;/kylin/api/cubes/&quot; + cubeName + &quot;/segments&quot;);JSONObject result = JSONObject.parseObject(str);JSONObject data = result.getJSONObject(&quot;data&quot;);JSONArray array = data.getJSONArray(&quot;segments&quot;);int size = array.size();for (int i = 0; i &lt; size; i++) { JSONObject segment = array.getJSONObject(i); ... String status = segment.getString(&quot;status&quot;); if (// 分区时间跟提交的任务构建范围一致) { if (status.equals(&quot;READY&quot;)) { // 已经构建，需要刷新 ... } else { // 相同分区正在构建，继续检查状态 ... } } else if (// 分区在提交的任务构建范围内) { // 存在子分区，刷新构建 ... } else if (// 分区时间与构建范围有交叉) { // 已经构建，但分区时间不匹配，退出执行 ... }} 6.2 提交构建任务通过kylin api，指定参数提交构建任务 123456789101112131415161718JSONObject json = new JSONObject();json.put(&quot;startTime&quot;, startTime);json.put(&quot;endTime&quot;, endTime);json.put(&quot;buildType&quot;, &quot;BUILD&quot;);json.put(&quot;project&quot;, projectName);// 1. 发起构建任务String str = KAPRestAPIUtil.sendPut(CommonUtils.KYLIN_HOST + &quot;/kylin/api/cubes/&quot; + cubeName + &quot;/segments/build&quot;, json.toJSONString());JSONObject result = JSONObject.parseObject(str);String code = result.getString(&quot;code&quot;);JSONObject data = result.getJSONObject(&quot;data&quot;);if (code.equals(&quot;000&quot;)) { logger.info(&quot;构建任务提交成功&quot;);} else { logger.error(&quot;构建任务提交失败:&quot; + result.getString(&quot;msg&quot;)); System.exit(-1);} 6.3 检测任务状态通过kylin api，获取cube构建任务的状态 123456789101112String str = KylinRestAPIUtil.sendGet(CommonUtils.KYLIN_HOST + &quot;/kylin/api/jobs/&quot; + uuid);JSONObject result = JSONObject.parseObject(str);status = result.getString(&quot;job_status&quot;);String progress = result.getString(&quot;progress&quot;).split(&quot;\\\\.&quot;)[0] + &quot;%&quot;;if (status.equals(&quot;FINISHED&quot;)) { // 任务完成} else if (status.equals(&quot;PENDING&quot;) || status.equals(&quot;RUNNING&quot;)) { // 任务运行中，继续检测状态} else if (status.equals(&quot;DISCARDED&quot;) || status.equals(&quot;ERROR&quot;) || status.equals(&quot;STOPPED&quot;)) { // 任务失败，获取失败步骤日志} else { // 其他异常 7. 使用示例7.1 配合Crontab123crontab -e// 每天0点执行,构建T+1的cube kylin_sales0 0 * * * java -jar kylin_cube_utils.jar kylin_sales day `date +%Y-%m-%d` 7.2 运行日志1暂无，待补充 8. 智能调度工具带来的好处结合contrl-M、crontab等调度工具，传入Cube名称、构建类型和构建日期，定时调用智能构建程序，自动处理构建中的各种情况，在出现异常时，也能很好的获取异常日志，无需后台查询，提高了Kylin的使用效率。","link":"/kylin-cube-build-tools.html"},{"title":"mapreduce性能优化","text":"基本 要减少网络延迟，配置所有节点在同一子网 不使用虚拟机 noatime挂载磁盘，文件访问数据不会被记录 节点磁盘不要做RAID和LVM map和reduce task设置 设置map执行结束之后再执行reduce任务，mapreduce.job.reduce.slowstart.completedmaps设置成1 如果每个task的执行时间少于30到40秒，就减少task的数量。Task的创建与调度一般耗费几秒的时间，如果task完成的很快，我们就是在浪费时间。同时，设置JVM重用也可以解决这个问题。 如果一个job的输入数据大于1TB，我们就增加block size到256或者512，这样可以减少task的数量。你可以使用这个命令去修改已存在文件的block size: hadoop distcp -Ddfs.block.size=$[25610241024] /path/to/inputdata /path/to/inputdata-with/largeblocks。在执行完这个命令后，你就可以删除原始的输入文件了(/path/to/inputdata)。 不要调度太多的reduce task — 对于大多数job来说，我们推荐reduce task的数量应当略小于cluster中reduce slot的倍数 设置LZO压缩，压缩reduce输出结果，因为副本数为3，压缩输出可以节省空间和提高性能，同时设置mapred.compress.map.output值为true。 程序编写优化 使用最合适最简洁的writable类型，大数据量的数值和文本间的转换会消耗大量CPU时间 重用writable类型，避免重复的new Text” 或”new IntWritable 使用StringBuffer.append来连接字符串 技巧 如果是测试环境，可以取消hadoop hdfs的用户权限检查。打开conf/hdfs-site.xml，找到dfs.permissions属性修改为false（默认为true）OK了。 timeout 设置大点，避免长时间的任务超时而停止执行","link":"/backup_posts/mapreduce%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.html"},{"title":"nba2016赛季前瞻","text":"今天NBA 2015-2016赛季常规赛开始了，这注定又是一个精彩的赛季。让我来盘点下新赛季的值得关注的地方。 库里上赛季的MVP和总冠军，常规赛揭幕战，首节13投9中砍下24分，三节40分，新赛季卫冕之路值得期待！ 科比科比，尽管休战了这么长时间，但他的出现，还是光芒万丈，这个赛季能进季后赛，湖人就算成功的！ 这是告别赛季，没有科比的NBA，少了太多。 书豪季前赛，打出几场队中MVP表现，新赛季，新球队，希望书豪能在乔帮主麾下展现自我。 詹姆斯James足够伟大，别又是亚军。 其他看点 Chris Paul：打入西决？ Kevin Durant：尝尝总冠军滋味？ James Harden ：火箭升空？ Rose：东部登顶？ - -漫长的常规赛，考验的是球员和球队的韧性。","link":"/2016nba.html"},{"title":"nginx负载均衡","text":"nginx可以按照调度规则实现动态、静态页面的分离，也可以按照轮询、ip哈希、URL哈希、权重等多种方式对后端服务器做负载均衡，同时还支持后端服务器的健康检查。 nginx的upstream目前支持的5种方式的分配 1.轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 1234upstream backserver { server 192.168.0.14; server 192.168.0.15; } 2.指定权重 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 1234upstream backserver { server 192.168.0.14 weight=10; server 192.168.0.15 weight=10; } 3.IP绑定 ip_hash (session绑定) 每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 12345upstream backserver { ip_hash; server 192.168.0.14:88; server 192.168.0.15:80; } 4.fair（第三方） 按后端服务器的响应时间来分配请求，响应时间短的优先分配。 12345upstream backserver { server server1; server server2; fair; } 5.url_hash（第三方） 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 123456upstream backserver { server squid1:3128; server squid2:3128; hash $request_uri; hash_method crc32; } 在需要使用负载均衡的server中增加示例配置 123456789101112proxy_pass http://backserver/; upstream backserver{ ip_hash; server 127.0.0.1:9090 down; (down 表示单前的server暂时不参与负载) server 127.0.0.1:8080 weight=2; (weight 默认为1.weight越大，负载的权重就越大) server 127.0.0.1:6060; server 127.0.0.1:7070 backup; (其它所有的非backup机器down或者忙的时候，请求backup机器) } max_fails ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误fail_timeout:max_fails次失败后，暂停的时间","link":"/nginx-load-balancing.html"},{"title":"shell逐行读取文本的几种方法","text":"经常会对文体文件进行逐行处理，在Shell里面如何获取每行数据，然后处理该行数据，最后读取下一行数据，循环处理．有多种解决方法如下： 通过read命令完成．read命令接收标准输入，或其他文件描述符的输入，得到输入后，read命令将数据放入一个标准变量中． 利用read读取文件时，每次调用read命令都会读取文件中的”一行”文本． 当文件没有可读的行时，read命令将以非零状态退出． 复制代码 123456789cat data.dat | while read linedo echo &quot;File:${line}&quot;done while read linedo echo &quot;File:${line}&quot;done &lt; data.dat 2.使用awk命令完成 awk是一种优良的文本处理工具，提供了极其强大的功能． 利用awk读取文件中的每行数据，并且可以对每行数据做一些处理，还可以单独处理每行数据里的每列数据． 12cat data.dat | awk '{print $0}'cat data.dat | awk 'for(i=2;i&lt;NF;i++) {printf $i} printf &quot;\\n&quot;}' 第1行代码输出data.dat里的每行数据，第2代码输出每行中从第2列之后的数据． 如果是单纯的数据或文本文件的按行读取和显示的话，使用awk命令比较方便． 3.使用for var in file 命令完成 for var in file 表示变量var在file中循环取值．取值的分隔符由$IFS确定． 123456789for line in $(cat data.dat)do echo &quot;File:${line}&quot;done for line in `cat data.dat`do echo &quot;File:${line}&quot;done 如果输入文本每行中没有空格，则line在输入文本中按换行符分隔符循环取值． 如果输入文本中包括空格或制表符，则不是换行读取，line在输入文本中按空格分隔符或制表符或换行符特环取值． 可以通过把IFS设置为换行符来达到逐行读取的功能． IFS的默认值为：空白(包括：空格，制表符，换行符)． 字符串split获取到每行的文本字符串，也许还会用到字符串的切分，可以使用以下方法： 12345tmp_str=&quot;a,b,c&quot;table=(${tmp_str//,/ })table_name=${table[1]}cube_name=${table[0]}echo $table_name $cube_name","link":"/shell-row.html"},{"title":"shell通用日志函数","text":"为了方便调试，写了一个通用的日志函数，实现功能： 设定日志级别，实现可以输出不同级别的日志信息 日志格式类似为：[日志级别] 时间 funcname:函数名 [lineno:行号] 日志信息 不同级别，设定不同颜色 代码: 123456789101112131415function log { local text;local logtype logfile=/var/log/mylog/mylog.log logtype=$1 text=$2 message=&quot;`date +'%F %H:%M:%S'` $1 line:$LINENO $2&quot; case $logtype in error) echo -e &quot;\\033[31m${message}\\033[0m&quot; | tee -a $logfile;; info) echo -e &quot;\\033[32m${message}\\033[0m&quot; | tee -a $logfile;; warn) echo -e &quot;\\033[33m${message}\\033[0m&quot; | tee -a $logfile;; esac} 使用: 123log info &quot;this's info log&quot;log error &quot;this's error log&quot;log warn &quot;this's warn log&quot; 注: 12345local : 局部变量$LINENO：shell 脚本行数$1 $2 ：函数参数tee 输出到控制台和文件\\033[31m \\033[0m : echo 输出颜色","link":"/shell-log-function.html"},{"title":"typecho使用pjax","text":"传统ajax，只能异步获取到数据，但是不能动态的改变网页地址。 pjax即ajax+history.pushState HTML5里引用了新的API，history.pushState和history.replaceState，就是通过这个接口做到无刷新改变页面URL的。 最典型的应用网站是github.com，本站也加入了pjax 我使用的是welefen封装过的pjax的jQuery版本 同时支持了缓存和本地存储，下次访问的时候直接读取本地数据，无需在次访问。 并且展现方式支持动画技术，可以使用系统自带的动画方式，也可以自定义动画展现方式。 使用步骤1. 下载jquery.pjax.js下载地址引用下载到的jquery.pjax.js到header.php中 2. 对a链接进行使用pjax绑定 12345678910111213141516171819202122232425262728293031323334353637383940414243//本站实例jQuery(document).ready(function(){ var $=jQuery; //绑定链接 $.pjax({ selector: &quot;a[href^='http://iyanlei.com'][href$='.html']&quot;, container: '.ajaxdiv', //内容替换的容器 show: 'slide', //展现的动画，支持默认和fade, 可以自定义动画方式，这里为自定义的function即可。 cache: false, //是否使用缓存 storage: true, //是否使用本地存储 titleSuffix: ' | Ray', //标题后缀 filter: function(){}, callback: function(status){ $(&quot;#nav-menu&quot;).addClass(&quot;animated fadeInUp&quot;); } }); //绑定跳转开始事件 $(&quot;.ajaxdiv&quot;).bind(&quot;pjax.start&quot;, function() { $(&quot;.ajaxdiv&quot;).css(&quot;opacity&quot;,&quot;0.6&quot;); $(&quot;.spinner&quot;).css(&quot;opacity&quot;,&quot;1&quot;); $(&quot;.spinner&quot;).show(); }); //绑定跳转结束事件 $(&quot;.ajaxdiv&quot;).bind(&quot;pjax.end&quot;, function() { $(&quot;.spinner&quot;).hide(); $(&quot;.ajaxdiv&quot;).css(&quot;opacity&quot;,&quot;1&quot;); // Main initHeader(); addListeners(); if (navigator.userAgent.indexOf('Firefox') &gt;= 0){ document.documentElement.scrollTop=120; } else { $('body').animate({scrollTop: 120}); } }); }); 3. typecho后端修改因为pjax使用的还是AJAX,所以 异步请求的时候后端不能将公用的内容也返回。即需要一个判断是否pjax请求的接口，判断如果是pjax请求时，仅返回公共的数据即可在主题下的functions.php中声明如下函数 123function is_pjax(){ return array_key_exists('HTTP_X_PJAX', $_SERVER) &amp;&amp; $_SERVER['HTTP_X_PJAX']; } 4. typecho页面修改可以在head.php和footer.php等的页面中添加判断，如： 1234//请在公共部分加入下面的判断&lt;?php if (!is_pjax()) { ?&gt;... //公共数据&lt;?php } ?&gt; 参考资料HTML 5 Web 存储pjaxHTML5/history","link":"/typecho-pjax.html"},{"title":"ycsb使用指南","text":"YCSB（Yahoo! Cloud Serving Benchmark）是雅虎开源的一款通用的性能测试工具。通过这个工具我们可以对各类NoSQL产品进行相关的性能测试，包括：PNUTS、BigTable 、HBase、Hypertable、Azure、Cassandra、CouchDB、Voldemort、MongoDb、Dynomite。 YCSB与HBase自带的性能测试工具（PerformanceEvaluation）相比，好处在于： 扩展：进行性能测试的客户端不仅仅只是HBase一款产品，而且可以是HBase不同的版本。 灵活：进行性能测试的时候，可以选择进行测试的方式：read+write，read+scan等，还可以选择不同操作的频度与选取Key的方式。 监控：进行性能测试的时候，可以实时显示测试进行的进度： 下载下载ycsb 0.6压缩包，解压即可https://github.com/brianfrankcooper/YCSB/releases/download/0.6.0/ycsb-0.6.0.tar.gz 注：执行ycsb时，如果报python argparse模块异常，因为python版本低，需要安装python argparse模块 ，解压进入执行： 1python setup.py install 创建Hbase表 12n_splits = 200 create 'usertable', 'family', {SPLITS =&gt; (1..n_splits).map {|i| &quot;user#{1000+i*(9999-1000)/n_splits}&quot;}} 参数配置Ycsb的执行配置在workload文件下，可以参考workloada和workload_template配置文件 Load配置Workload文件夹下新建workload_load文件 1234567891011121314151617181920212223#记录数recordcount=10000000#操作记录数operationcount=10000000# 测试类workload=com.yahoo.ycsb.workloads.CoreWorkloadreadallfields=true# read比例readproportion=0# update比例updateproportion=0# scan比例scanproportion=0# Load比例insertproportion=1requestdistribution=zipfian# The number of fields in a recordfieldcount=10# The size of each field (in bytes)fieldlength=100# hbase表table=usertablecolumnfamily=family Scan配置Workload文件夹下新建workload_scan文件 12345678910111213recordcount=10000000operationcount=10000000workload=com.yahoo.ycsb.workloads.CoreWorkloadreadallfields=truereadproportion=0updateproportion=0scanproportion=1insertproportion=0requestdistribution=zipfiantable=usertablecolumnfamily=family#必填hbase.usepagefilter=false Read配置Workload文件夹下新建workload_read文件 1234567891011recordcount=10000000operationcount=10000000workload=com.yahoo.ycsb.workloads.CoreWorkloadreadallfields=truereadproportion=1updateproportion=0scanproportion=0insertproportion=0requestdistribution=zipfiantable=usertablecolumnfamily=family 测试 Load1bin/ycsb load hbase098 -P workloads/workload_load -cp hbase098-binding/conf –threads 100 Scan1bin/ycsb load hbase098 -P workloads/workload_scan -cp hbase098-binding/conf –threads 100 Read1bin/ycsb load hbase098 -P workloads/workload_read -cp hbase098-binding/conf –threads 100 测试的时候可以在集群的节点上启多个客户端执行ycsb测试，如：Load测试，集群5个节点，各启动10个客户端，每个客户端启动100个线程，总共是5000个线程执行Load","link":"/backup_posts/ycsb%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97.html"},{"title":"读阿里开发手册","text":"阿里作为全世界Java开发规模最大和应用水平最高的地方之一，三十几页的规范，从编程、异常日志、SQL、工程、安全五块形成的规约总结，根据约束力，分强制、推荐、参考三大类，可以作为优秀技术团队的重要利器。 官方文档地址：https://yq.aliyun.com/articles/69327?utm_content=m_10088 以下是阅读阿里开发手册中，记录的一些重要摘要信息。 编程规范命名 尽量语义清晰、望名知义 遵从驼峰形式 如果使用到了设计模式，类名体现具体模式 常量定义 值定义不要出现未定义的常量 常量类推荐也分类定义常量类，不要大而全的一个常量类 变量值在一定范围，使用Enum类 格式规约 单行限制不超过120个字符 看注释内容，示例代码： 123456789101112131415161718public static void main(String[] args) { // 缩进4个空格 String say = &quot;hello&quot;; // 运算符的左右必须有一个空格 int flag = 0; // 关键词if与括号之间必须有一个空格，括号内的f与左括号，0与右括号不需要空格 if (flag == 0) { System.out.println(say); } // 左大括号前加空格且不换行；左大括号后换行 if (flag == 1) { System.out.println(&quot;world&quot;); // 右大括号前换行，右大括号后有else，不用换行 } else { System.out.println(&quot;ok&quot;); // 在右大括号后直接结束，则必须换行 }} OOP规约 overwrite方法，必须有@overwrite注解 静态方法和变量，直接通过类名访问，禁止对象访问，增加无谓的编译器解释成本 不能使用过时的类或方法 包装类之间的值比较，使用equals方法 构造方法禁止加入任何业务逻辑，请放在init方法中 String的split方法，做分隔符后无内容的检测 类方法定义的顺序，公有方法或保护方法 &gt; 私有方法 &gt; getter/setter方法 getter/setter方法尽量不要加入任何业务逻辑，增加排查问题的难度 推荐使用StringBuilder的append方法替换string拼接 集合处理并发处理 线程定义名称 使用线程池，使用ThreadPoolExecutor 高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁；能锁区块，就不要锁整个方法体；能用对象锁，就不要用类锁。 使用CountDownLatch进行异步转同步操作，每个线程退出前必须调用countDown方法，线程执行代码注意catch异常，确保countDown方法可以执行，避免主线程无法执行至await方法，直到超时才返回结果。 控制语句1.推荐尽量少用else， if-else的方式可以改写成： 12345if (condition) { ... return obj;}// 接着写else的业务逻辑代码; a 注释 类、属性、方法，使用/*内容/定义注释，不得使用//注释 抽象方法、枚举，详细注释 注释掉的代码，配合说明 异常日志异常处理 RuntimeException预先检查进行规避，避免IndexOutofBoundsException/NullPointerException 异常不能用来做流程控制、条件控制，比条件判断效率低 finally需要关闭资源、流、连接等 避免出现重复代码 日志 使用SLF4J日志框架的API，不直接使用log4j/logback 日志文件命名，含义清晰 异常信息，包含现场信息和堆栈信息1logger.error(各类参数或者对象toString + &quot;_&quot; + e.getMessage(), e); 使用条件输出或占位输出123456正例：（条件）if (logger.isDebugEnabled()) { logger.debug(&quot;Processing trade with id: &quot; + id + &quot; symbol: &quot; + symbol);}正例：（占位符）logger.debug(&quot;Processing trade with id: {} symbol : {} &quot;, id, symbol); MySQLMySQL规约中，MySQL使用方法都是比较有用的经验之选，推荐都仔细研读。 建表 使用小写字母，数字，下划线命名 尽量不修改表名、字段名 主键pk_ ,索引 idx_ ,唯一 uk_ 小数类型为decimal，禁止使用float和double。 如果存储的字符串长度几乎相等，使用char定长字符串类型 索引 join字段类型一致、禁止超过3表join 在varchar字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度即可。 说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为20的索引，区分度会高达90%以上，可以使用count(distinct left(列名, 索引长度))/count(*)的区分度来确定。 页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。右模糊可以利用b+树最左前缀匹配 利用延迟关联或者子查询优化超多分页场景。 说明：MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行SQL改写。 正例：先快速定位需要获取的id段，然后再关联： SELECT a.* FROM 表1 a, (select id from 表1 where 条件 LIMIT 100000,20 ) b where a.id=b.id SQL规约 【强制】不要使用count(列名)或count(常量)来替代count()，count()是SQL92定义的标准统计行数的语法，跟数据库无关，跟NULL和非NULL无关。 说明：count(*)会统计值为NULL的行，而count(列名)不会统计此列为NULL值的行。 【强制】count(distinct col) 计算该列除NULL之外的不重复行数，注意 count(distinct col1, col2) 如果其中一列全为NULL，那么即使另一列有不同的值，也返回为0。 【强制】当某一列的值全是NULL时，count(col)的返回结果为0，但sum(col)的返回结果为NULL，因此使用sum()时需注意NPE问题。 ORM规约 在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。 @Transactional事务不要滥用。事务会影响数据库的QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。 工程规约应用分层 图中默认上层依赖于下层，箭头关系表示可直接依赖，如：开放接口层可以依赖于Web层，也可以直接依赖于Service层，依此类推： 开放接口层：可直接封装Service方法暴露成RPC接口；通过Web封装成http接口；进行网关安全控制、流量控制等。 终端显示层：各个端的模板渲染并执行显示的层。当前主要是velocity渲染，JS渲染，JSP渲染，移动端展示等。 Web层：主要是对访问控制进行转发，各类基本参数校验，或者不复用的业务简单处理等。 Service层：相对具体的业务逻辑服务层。 Manager层：通用业务处理层，它有如下特征： 1） 对第三方平台封装的层，预处理返回结果及转化异常信息； 2） 对Service层通用能力的下沉，如缓存方案、中间件通用处理； 3） 与DAO层交互，对多个DAO的组合复用。 DAO层：数据访问层，与底层MySQL、Oracle、Hbase进行数据交互。 外部接口或第三方平台：包括其它部门RPC开放接口，基础平台，其它公司的HTTP接口。 服务器规约 给JVM设置-XX:+HeapDumpOnOutOfMemoryError参数，让JVM碰到OOM场景时输出dump信息。 高并发服务器建议调小TCP协议的time_wait超时时间。 安全规约用户请求传入的任何参数必须做有效性验证。 说明：忽略参数校验可能导致： page size过大导致内存溢出 恶意order by导致数据库慢查询 任意重定向 SQL注入 反序列化注入 正则输入源串拒绝服务ReDoS 附录分层领域模型规约 DO（Data Object）：与数据库表结构一一对应，通过DAO层向上传输数据源对象。 DTO（Data Transfer Object）：数据传输对象，Service和Manager向外传输的对象。 BO（Business Object）：业务对象。可以由Service层输出的封装业务逻辑的对象。 QUERY：数据查询对象，各层接收上层的查询请求。注：超过2个参数的查询封装，禁止使用Map类来传输。 VO（View Object）：显示层对象，通常是Web向模板渲染引擎层传输的对象。 专有名词 POJO（Plain Ordinary Java Object）：在本规约中，POJO专指只有setter/getter/toString的简单类，包括DO/DTO/BO/VO等。 DO（Data Object）：本手册指数据库表一一对应的POJO类。 GAV（GroupId、ArtifactctId、Version）：Maven坐标，是用来唯一标识jar包。 OOP（Object Oriented Programming）: 本手册泛指类、对象的编程处理方式。 ORM（Object Relation Mapping）: 对象关系映射，对象领域模型与底层数据之间的转换，本文泛指iBATIS, mybatis等框架。 NPE（java.lang.NullPointerException）: 空指针异常。 一方库：本工程内部子项目模块依赖的库（jar包）。 二方库：公司内部发布到中央仓库，可供公司内部依赖的库（jar包）。 三方库：公司之外的开源的依赖库（jar包） 官方文档地址：https://yq.aliyun.com/articles/69327?utm_content=m_10088","link":"/alidevnotes.html"},{"title":"一首英文歌","text":"123456hiding from the rain and snow trying to forget but I won't let go looking at a crowded street listening to my own heart beat so many people all around the world ..... 晚饭的时候，当餐厅缓缓响起这首《take me to your heart》. 随着这首美妙的歌曲渐入高潮，我随之吟唱起来，也勾起了一段高中时代的小回忆，同时向朋友述说着… 2004年，还是在高一的时候，当Michael Learns to Rock发行《take me to your heart》这首歌曲，我们的英语老师觉得很好听，把歌词抄给我们记下，每到英语课，都会花几分钟，用磁带播放机一句一句播放，一句一句的教给我们，那个时候也是上英语课最开心的时刻，很快我们就会了，至今都能唱起。 开心时光并没有延续太长时间，不久，英语老师被诊断出了胃癌晚期。 然后英语老师缺席了我们后面的课程。 学校发动了募捐，我们也经常组织探望老师，但这并没有改变什么。 同学期，英语老师最终还是未能战胜病魔。《take me to your heart》也成了我们对老师最后的回忆。 我想，如果放在现在，如果能早点发现，那并没有那么多如果。对于不可逆的事情，你唯一能做也许就是令自己不要忘记。 过去10多年，这首歌但一直记忆深刻，《take me to your heart》也成了我最拿手的英文歌曲。 我想，这也是我记忆最深的一首歌。 《take me to your heart》是迈克学摇滚（Michael Learns to Rock）主唱的一首歌，此歌曲翻唱自张学友的《吻别》。收录于专辑《Take Me to Your Heart》，2004年发行。 迈克学摇滚（Michael Learns To Rock），始建于1987年，曾被形容是斯堪的纳维亚音乐传统与西洋流行音乐的相遇，绝非「丹麦团」三个字就可以解释。他们全球千万张惊人的销售量是一般乐团少有的，迈克学摇滚深入慢摇音乐情感层面并从中精确掌握音符组构成果的能力，使得他们成为丹麦音乐史上最成功的团体之一。迈克学摇滚在家乡已经有超过五十万张唱片的销售，但更惊人的是他们在全球的销售数字更是高达千万张之高！其中在亚洲更是掀起一阵狂潮。","link":"/a-english-song.html"},{"title":"动态加载多说评论","text":"由于博客使用了异步加载，本来想用angularjs，但angularjs对于一个小博客来说有点重了，所以还是使用了pushState + Ajax(pjax)，于是多说也需要实现动态加载。 多说代码 引入多说JS 点击按钮，展示多说评论 123456789101112131415161718192021222324&lt;span id=&quot;expandComments&quot; onclick=&quot;toggleDuoshuoComments('#comment-box');&quot;&gt;展开评论&lt;/span&gt;&lt;div id=&quot;comment-box&quot; &gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt;var duoshuoQuery = {short_name:&quot;misray&quot;};function toggleDuoshuoComments(container){ $(&quot;#expandComments&quot;).hide(); var el = document.createElement('div');//该div不需要设置class=&quot;ds-thread&quot; el.setAttribute('data-thread-key', '&lt;?php echo $post_id ;?&gt;');//必选参数 el.setAttribute('data-url', '&lt;?php the_url(); ?&gt;');//必选参数 el.setAttribute('data-author-key', 'misray');//可选参数 el.setAttribute('data-title', '&lt;?php the_title(); ?&gt;');//可选参数 DUOSHUO.EmbedThread(el); jQuery(container).append(el);}(function() { var ds = document.createElement('script'); ds.type = 'text/javascript';ds.async = true; ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js'; ds.charset = 'UTF-8'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);})();&lt;/script&gt; CSS 样式###123456789101112131415#expandComments{ padding: 8px 15px; border: 1px solid #ff5e52; text-decoration: none; cursor: hand; border-color: #ff5e52; color:#fff; border-radius: 10px; background-color: #ff5e52;}#expandComments:hover{ color:#ff5e52; border: 1px solid #ddd; background-color: #fff;}","link":"/duoshuo.html"},{"title":"书单","text":"","link":"/booklist.html"},{"title":"实现shell并发","text":"经常在工作中遇到执行的脚本需要并发执行，提高效率。 常规脚本 1234567#!bin/shfor((i=1;i&lt;=5;i++));do{ sleep 3; echo $i}done 执行：time bash test.sh执行结果：总共耗时15秒 12345678910111213[root@pc disk1]# time sh test.sh12345real 0m15.010suser 0m0.003ssys 0m0.005s[root@pc disk1]# 并发脚本 123456789#!bin/shfor((i=1;i&lt;=5;i++));do{ sleep 3; echo $i}&amp;donewaitecho 'over' 每次for循环后加了一个后台执行&amp;符号，那5个循环任务会并发执行,执行：time bash concurrent.sh执行结果：总耗时3秒 123456789101112[root@pc disk1]# time sh concurrent.sh 12354overreal 0m3.004suser 0m0.001ssys 0m0.002s[root@pc disk1]# wait的作用：等待前面后台执行的任务全部结束再往下执行，否则程序会直接执行结束。当然后台执行的脚本还会继续执行，只不过会影响wait后续的代码。","link":"/shell-concruent.html"},{"title":"大数据技术文章索引","text":"存储格式 深入分析Parquet列式存储格式 大数据开源列式存储引擎Parquet和ORC Hive:ORC File Format存储格式详解 文件系统 HDFS 原理、架构与特性介绍 HDFS的运行原理 Namenode HA原理详解 Apache Kudu 加速对频繁更新数据的分析 基于内存的统一分布式存储系统 Alluxio 及其应用 计算引擎 MapReduce框架详解 MapReduce:详解Shuffle过程 Spark运行原理 Spark性能优化指南 新一代大数据处理引擎 Apache Flink Apache Flink：特性、概念、组件栈、架构及原理分析 资源管理任务调度 Hue安装配置实践 Azkaban 作业调度 NoSQL [HBase] LSM树 VS B+树 HBase在阿里的应用实践 SQL引擎 Impala：新一代开源大数据分析引擎 Apache Kylin的快速数据立方体算法——概述 Presto实现原理和美团的使用实践 流处理 Apache Storm内部原理分析 消息队列 消息队列设计精要 Kafka深度解析 Kafka文件存储机制那些事 数据采集 Flume NG 简介及配置实战 Flume日志收集分层架构应用实践 Sqoop详解 基于Flume的美团日志收集系统(一)架构和设计 算法案例 Apache Kylin在美团数十亿数据OLAP场景下的实践 Spark在美团的实践 携程实时用户行为系统实践 全文检索 Elasticsearch架构原理 solr vs elasticsearch 功能比较 机器学习 Apache Mahout 简介 其他 谷歌新发布的分布式数据库服务，是要打破CAP定理了吗？ 分布式(hadoop)内核研发面试指南","link":"/bigdata-tech-post-index.html"},{"title":"我的博客史","text":"说起我的博客历史，大概分为6段小历史，首先得从2010年说起。 12010年，从大学毕业一开始就已经开始了我的博客生涯。当时第一个博客还是在Iteye上，当时的博客地址是：帅的被神砍 ，寥寥写了几篇技术文章。当时每天必上的技术网站就是Iteye，关注创始人的robbin的动态，然而iteye已经被csdn收购，robbin也已经成了丁香园的CTO 2做为程序员，最关注的就是开源，所以iteye的衰弱，让我从iteye转到了oschina，转发或原创了一些技术文章。当时比较开始的是，一有用到的精简代码，就会在上面分享出来。磊神Ray分享的代码列表(32)，因为当时分享的代码在开源中国首页，每天都有很多人看和讨论。 3不满足与在技术网站上建博客，于是开始自建博客，买的第一个域名是iyanlei.com，这个博客也一直保留了下来。用的typecho，写了不少typecho主题。也分享了一些typecho的技术文章，如typecho pjax的实现。 4工作的原因，很久没更新iyanlei.com，同时觉得域名不好看，于是购买了misray.com，用的wordpress，做了个主题，写了一些生活类的文章。 5又开始专注技术了，入了infullstack.com，也分享了不少技术文章。 终最终回归到了misray.com，把以前所有值得收藏的文章，统一了一下，用github+hexo，继续着我的博客之路。 总结了一些博客之路的心得：简单的就是最美的，内容为王。","link":"/backup_posts/%E6%88%91%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%8F%B2.html"},{"title":"常用的linux命令","text":"Linux kill多个进程1ps -ef|grep ycsb|grep -v grep|cut -c 9-15|xargs kill -9 列举根目录文件下文件大小1du / -m --max-depth 1 |sort -nr 查看端口占用12netstat -anp|grep port即：netstat –apn | grep 8080 查找文件内容1grep -r magic /usr/src #显示/usr/src目录下的文件(包含子目录)包含magic的行 使用混合查找方式查找文件1find /tmp -size +10000c -and -mtime +2 #在/tmp目录下查找大于10000字节并在最后2分钟内修改的文件 Linux光盘挂载1mount -t iso9660 -o loop CentOs.iso /mnt/cdrom 根据 CPU 使用来升序排序1ps -aux --sort -pcpu | less for host in VM-233-25-centos VM-226-228-centos VM-252-84-centos VM-232-137-centos VM-251-49-centos VM-249-233-centos VM-101-129-centos VM-101-234-centos; do ssh $host yum clean all; done","link":"/linux-command.html"},{"title":"收集jmap和jstack信息","text":"收集jmap和jstack信息的方法 12345678910111213141516171819202122232425262728293031323334353637383940414243#收集jmap信息#!bin/shif [ $# -lt 2 ] ; then echo &quot;please input port and execute count parameter&quot; exitfijmap_dir=/mnt/ray/jmap/jstack_dir=/mnt/ray/jstack/port=$1count=$2i=1while(($i&lt;$count))do filename=`date '+%Y-%m-%d_%H:%M:%S'` sudo -u hive /usr/java/latest/bin/jmap -histo $port &gt; $jmap_dir$filename&quot;_&quot;$port.jmap sudo -u hive /usr/java/latest/bin/jmap -heap $port &gt;&gt; $jmap_dir$filename&quot;_&quot;$port.jmap sudo -u hive /usr/java/latest/bin/jstack $port &gt; $jstack_dir$filename&quot;_&quot;$port.jstack i=$(($i+1)) sleep 1mdone#收集jstack信息#!bin/shjmap_dir=/mnt/ray/jmap/jstack_dir=/mnt/ray/jstack/count=$1i=1mkdir /mnt/ray/jmap/mkdir /mnt/ray/jstack/while(($i&lt;$count))do for port in $(jps|grep CoarseGrainedExecutorBackend|awk '{print $1}') do filename=`date '+%Y-%m-%d_%H:%M:%S'` sudo -u hive /usr/java/latest/bin/jmap -histo $port &gt; $jmap_dir$filename&quot;_&quot;$port.jmap sudo -u hive /usr/java/latest/bin/jmap -heap $port &gt;&gt; $jmap_dir$filename&quot;_&quot;$port.jmap sudo -u hive /usr/java/latest/bin/jstack $port &gt; $jstack_dir$filename&quot;_&quot;$port.jstack i=$(($i+1)) sleep 1m done done 1nohup sh jmap.sh 16019 &amp;","link":"/collect-jmap-jstack.html"},{"title":"有效的管理邮件","text":"邮件，作为工作中很重要的东西。工作多年，有自己使用的一些方法和技巧。最近也参加了一个知乎关于邮件处理的live，总结下关于邮件处理的应该注意的一些心得和技巧，合理的利用这些在短期内提高管理电子邮件的效率。 Zero InboxZero Inbox，意为零收件箱。对每封邮件做到当天处理，保持收件箱清空状态。 将立即回复和定时回复结合起来，对重要联系人（客户、领导）发来的邮件，或者包含重要事情的邮件，迅速采取相应的行动。其他邮件，可以在一天中选择固定的时间进行统一处理。 在结束一天工作时，让收件箱空空如也，保证所有的事情都被处理妥当，这不是一个容易养成的习惯。却是可以带来很大的收获，让每天的工作更严谨效率。 用好过滤器设置邮件规则，在收到或发送一封邮件时，能够指挥邮箱对每封邮件进行一系列的条件判断，再根据预设的动作，对邮件进行处理。自定义常用的规则，达到自己满意的效果。 过滤器常用的条件包含： 发件人和收件人是谁 主题或正文包含什么关键字 过滤器预设的动作包含： 转移、自动回复、删除 标记已读 打标签 对每封邮件进行4D处理4D指，行动(Do)，转发(Delegate)，搁置(Defer)，删除（Delete） 行动 当发现邮件内容是由你来完成，且能在短时间内完成，则可以立即采取行动。 转发 当发现邮件有更适合的人去完成，或更低成本的完成的时候，可以选择转发。 搁置 当前有重要事情在做，且该邮件内容不能短时间完成，则可以放入到搁置文件夹中。 删除 可以把通知类邮件，放入 已处理 或 通知 文件夹。 写邮件技巧掌握并是用这些技巧，真正言简意赅、目的明确的表达邮件内容，写出高质量的邮件，让收件人处理邮件更为轻松。 邮件模版 配置常用的邮件模版，比如签名档 善于利用标题 增加&lt;信息&gt; &lt;请求&gt; &lt;行动&gt; &lt;周报&gt; &lt;日报&gt; &lt;培训&gt; &lt;重要&gt;等标注，能突出内容主题 长邮件提醒 对于长邮件，在邮件开头做说明，并提取重要信息阐述 长邮件结构清晰 利用标题、颜色、加粗、序号等方式，清晰表达邮件内容 给收件人多种选择 如果需要收件人采取行动时，可以提供多种选择，避免邮件长时间没得到结果 重读邮件 每次发送前，重复读一下邮件内容，语句通顺，主题明确，逻辑清晰 总结行动要点 如果需要收件人采取行动，告知收件人行动要点。 一封邮件一个主题 尽量一封邮件只说一件事情，或者汇总相关的事情。 写好一封邮件ABC法则，是一个邮件的框架，帮助你把信息放在正确的位置，把内容分成三个部分，简述Action、背景Backgroud、结尾Close 简述Action 简明的阐述行动的目的和关键点、点明前因后果，做到准确明白 背景Backgroud 清楚、简洁、相关 结尾Close 阐述下一步行动和小提示、签名 示例： 写邮件跟写博客文章一样，需要经常总结和应用合适的方法和技巧，不日之后，即可顺手拈来。 本文内容多为网上关于邮件管理的经验之谈，这里做一个适合自己的邮件管理的汇总笔记。","link":"/email.html"},{"title":"浅谈2017NBA常规赛MVP","text":"16-17赛季常规赛接近尾声，西部季后赛球队名单和排名基本确定，反观东部，3-4名还在争夺，5/6/7/8/9还在激烈争夺。 常规赛MVP，这个赛季也是竞争相当激烈。 MVP候选浅析詹姆斯.哈登哈登场均29.1分8.1个篮板11.2次助攻，助攻居联盟第一，得分居联盟第二，火箭至今52胜25负，列西部第三，也是全联盟第三最佳战绩。 韦斯特布鲁克场均31.8分，10.7个篮板，10.4次助攻，雷霆战绩西部第六。 至今已经拿到41个三双，赛季场均三双已成事实，破罗伯特森记录也是指日可待。同时也是NBA史上得分最高的三双。 但也许会是个悲剧，因为哈登同样出色。就像2005-2006赛季的科比，当年科比以场均35.4分打破了NBA自1970年有纪录以来的最高得分，并且三次入选月最佳球员，轰下单场81分。但由于战绩不佳，最终评委把常规赛MVP给了纳什。 这个赛季的维斯很像独自带队时的科比，维斯身体素质和得分能力确实厉害，有很强的求胜欲望。能在他身上能找到科比的影子，却始终找不到科比的味道。 杜兰特场均25.3分，8.2个篮板，4.8次助攻，勇士战绩联盟第一。 伤病前，还在MVP排行前三，勇士战绩也一直占据联盟第一。个人不是很喜欢杜兰特来勇士，因为压缩了库里的表现。希望这个赛季能夺冠，不然就呵呵了。 詹姆斯场均26.3分，8.5个篮板，8.7次助攻，骑士战绩东部第一。 数据接近三双，几个赛季前，我就说过詹姆斯已经老了，然后看他这两个赛季的表现来说，并没有。也许不吃猪肉真的能保持好的身体。 库里场均25.3分，4.5个篮板，6.6次助攻，勇士战绩联盟第一。 相信，库里会是一个最伟大的投手。杜兰特的到来，占用不少球权。相对挤压了部分库里的表现，这个赛季，数据有点缩水。但勇士战绩连续三个赛季联盟第一。 莱昂纳德场均25.7分，5.9个篮板，3.6次助攻，马刺战绩西部第二。 越是低调的人，越可怕。莱昂纳德一直在成长，有莱昂纳德，马刺还可以再战十年。 伊赛亚.托马斯场均29.1分 2.7个篮板，5.7次助攻，凯尔特人战绩东部第二。 不愧是地表最强1米75，比1米83的艾弗森更强。很喜欢托马斯，因为他代表我们这类身高的人打篮球，同样可以征服NBA， 艾弗森也一直是自己的偶像。 约翰.沃尔场均23.2分，4.2个篮板，10.7次助攻，奇才战绩东部第三。 一直有关注沃尔，就是因为他风驰电掣般的运球速度。 总结赛季接近尾声，关于MVP，我认为只剩下三种可能了 哈登当选，没毛病 维斯微弱优势当选，评委无法拒绝场均三双不是MVP。 哈登维斯共同当选 当然，我对最后是哪种结果都不意外。 使用到的球员得分数据，来源：腾讯-NBA数据库","link":"/2017-nba-mvp.html"},{"title":"生成不重复随机数简单算法","text":"生成不重复随机数简单算法 12345678910111213141516171819202122232425262728import java.util.Arrays;import java.util.Random; /** * @author Ray 2011-9-8 */public class RandomTest { public static void main(String[] args) { // 声明一个种子 int seed[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9 }; // 存放生成后的数字 int[] destArray = new int[seed.length]; // 声明一个Random实例 Random random = new Random(); // 循环种子 for (int i = 0; i &lt; seed.length; i++) { // 随机得到种子中的一个位置 int j = random.nextInt(seed.length - i); // 把该位置上的种子输出 destArray[i] = seed[j]; // 把种子中末尾的种子替换得到的种子 seed[j] = seed[seed.length - 1 - i]; } System.out.println(Arrays.toString(destArray)); } }","link":"/non-repetition-algorithm.html"},{"title":"聊聊这几年的数据泄漏","text":"近几年，互联网数据泄漏事件频出， 事件发生的第一时间，往往我会下载泄漏数据，查看下自己是否在泄漏数据中。 下面就谈谈自己遇到的几次泄密事件。 CSDN数据泄漏2011年，CSDN遭遇到了一次重大的账户泄漏，黑客公开CSDN网站数据库 600余万用户资料泄密，所有账户密码都是明文存储。同时也开启了近几年国内互联网的泄漏风暴。 2010年就步入IT职场的我，毫无疑问，当时最火的IT社区就是javaeye和CSDN，所以我的账户也在泄漏数据之列。那个时候我的所有密码基本一致，虽然那个时候对我并没有造成过多的影响，但那时我就意识到帐号数据都处在很危险的地方。 相关阅读： 黑客公开CSDN网站数据库 600余万用户资料泄密 CSDN数据库泄露，大量用户真实账号密码外泄 汉庭如家2000万账户泄漏2013年10月，汉庭如家2000万账户泄密，庆幸的是，那个时候汉庭如家基本没住过，所以我的数据不在其中。 有趣的是，由于我从事大数据工作，在一次与某电科院交流时，客户提议，导入2000万文本数据到hive中，进行检索，测试我们的产品性能，片刻时间导入之后，输入某领导手机后，刷刷刷，多条开房记录在秒内查询出来了，把我们都笑晕了。 相关阅读： 如家等大量酒店客户开房记录被第三方存储并因漏洞导致泄露 QQ数据泄漏2013年，7000万个腾讯QQ群数据遭泄露，泄漏文件为SQL Server的数据库导出格式，“解压后达90多G，大概有7000多万个QQ群，12亿多个部分重复的QQ号码。 当然我的qq信息，也在其中，至今，在社工库都能查到我的相关信息。-_-!! 12306账户数据泄密在14年12月底，正是预定春运回家火车的时间，10几万条12306明文账户信息从网上公开，我立即下载下来，还好没有的帐号，随机选了几条数据登录12306，都能登录，当然我没有把别的票退掉。但这个的后果可想而知。 乌云网漏洞 网易邮箱其实，昨日就有大量网易邮箱遭全面暴力破解的相关新闻放出，大量网友称绑定的账号遭泄露。绑定网易邮箱的Apple ID被锁成砖，iPhone存储数据被清空。其中，包括苹果Apple ID在内的微博、支付宝、百度云盘等都受到影响。 还好，我的iphone先于帐号丢了。 -_- !! 相关阅读网易邮箱现巨大漏洞 过亿数据遭泄漏 小结当然，这些只是近几年较为重大的事件，其他泄漏事件也不少。http://www.wooyun.org/上经常会有帐号安全漏洞。 怎么规避账户泄漏用户无法避免账户泄漏，但需要确保不会引起连锁反应，所以我们最好控制每个账户的密码都不同，可以把密码设置为一段有含义的字母，好记又复杂。 推荐阅读： 一个密码改变了我的人生 我的银行密码设置方法卡号选几位 再利用各种运算符（加减乘除左移右移等）得出一个公式，算出一个密码。这样，只要有卡号，我就有密码了。(^__^) 嘻嘻","link":"/dataleakage.html"},{"title":"说说我的利器","text":"介绍一下你自己和所做的工作。## 我是Misray，89年生，是一名全栈工程师，现在在一家大数据公司任职研发，懂点大数据（hadoop、mapreduce、spark、flume）、JAVA、前端（html5、css3、js、angular js、nodejs ），样样通样样松，哈哈。 当然平时写写技术和生活博客、倒腾新奇技术，以前的博客在Ray，也弄了个订阅工具Feedly 你都在使用哪些硬件？##ThinkPad T440p，升级成了16内存 + 128 SSD + win 10，对于经常出差的我，开发和使用足矣 软件呢？PC软件GitHub Atom ,以前经常使用notepad和sublime text，但atom的出现，就换了。但atom的启动速度偏慢。 eclipse，JAVA开发的必备工具。当然也可以使用IntelliJ IDEA，使用eclipse只是习惯。 Clover，win下，浏览文件夹像使用chrome一样 红杏 和 shadowsocks ，不解释 PHPStudy ，开发PHP的利器 番茄土豆，番茄工作法 + GTD 效率工具 有道云笔记，够用的笔记工具 Xshell 、WinSCP APP软件知乎日报、好奇心、startupnews、开源中国、简书、uber、有道云笔记、掘金 你最理想的工作环境是什么？大木桌、人体学工程椅子、网速佳、空气环境好。 你平时获得工作灵感的方式有哪些？设计灵感网站 Dribbble 、 UI、 Next、 V2ex 技术开发网站 oschina开源中国 、cnblogs、 并发编程开发、importnews等 逛这些社区和网站是主要的灵感来源，网上看到喜欢的设计，会保存下来，在疲惫或毫无灵感时候欣赏，找到灵感。 推荐一件生活中的利器给大家。博朗或飞利浦的电动牙刷，保证口腔的干净对程序员十分重要。 本文参与了「利器社群计划」，发现更多创造者和他们的工具：利器","link":"/liqi.html"}],"posts":[{"title":"MPP架构和Batch架构","text":"在最早数据分析时代，我们用单机数据库系统来解决数据存储和分析需求，当数据量逐渐增加，就需要在架构上扩容，很自然首先就是通过增加计算资源(CPU、Memory)来提高分析性能，这就是常说的纵向扩展。 这个架构下，所有CPU都是共享磁盘和Memory，即资源都共享(Shared Everthing)。 很显然，这个架构是不可持续的，由于所有资源都共享，很容易产生竞争。为了解决这个问题，那就尽量少的共享。那就有shard disk/shard memory的架构，但很快也会得到的结论就是共享资源的竞争会阻碍扩展性。 于是就有了Shared Nothing的架构，于是就有了分布式系统。 数据库架构数据库构架设计中主要有Shared Everthting、Shared Nothing、和Shared Disk： Shared Everthting 一般是针对单个主机，完全透明共享CPU/MEMORY/IO，并行处理能力是最差的，典型的代表SQLServer Shared Disk 各个处理单元使用自己的私有 CPU和Memory，共享磁盘系统。典型的代表Oracle Rac， 它是数据共享，可通过增加节点来提高并行处理的能力，扩展能力较好。其类似于SMP（对称多处理）模式，但是当存储器接口达到饱和的时候，增加节点并不能获得更高的性能 。 Shared Nothing 各个处理单元都有自己私有的CPU/内存/硬盘等，不存在共享资源，类似于MPP（大规模并行处理）模式，各处理单元之间通过协议通信，并行处理和扩展能力更好。典型代表hadoop ，各节点相互独立，各自处理自己的数据，处理后的结果可能向上层汇总或在节点间流转。 我们常说的 Sharding 其实就是Share Nothing架构，它是把某个表从物理存储上被水平分割，并分配给多台服务器（或多个实例），每台服务器可以独立工作，具备共同的schema，比如MySQL Proxy和Google的各种架构，只需增加服务器数就可以增加处理能力和容量。 MPP 架构Batch 架构MPP 架构和 Batch 架构对比 MPP Hadoop 平台开放 封闭式和专有的。对于某些技术，甚至文档下载对于非客户是不可能的 完全开源，供应商和社区资源可在互联网上免费获得 硬件选项 许多解决方案都是专用的，您不能在自己的集群上部署软件。所有的解决方案都需要特定的企业级硬件，如快速磁盘、具有大量ECC内存的服务器、10GbE/Infiniband等。 任何硬件都可以工作，供应商提供了一些配置指南。大多数建议是在DAS中使用廉价的商品硬件 可伸缩性（节点） 平均10个节点最多100-200个 平均100个节点，几千个是最大值 可伸缩性（用户数据） 平均十兆字节PB是最大值 平均是几百TB最大是几十PB 查询延迟 10-20毫秒 10-20秒 查询平均运行时 5-7秒 10-15分钟 查询最大运行时 1-2小时 1-2周 查询优化 复杂的企业查询优化器引擎作为最有价值的企业秘密之一保存 没有优化器或功能非常有限的优化器，有时甚至不是基于成本的 查询调试和分析 代表查询执行计划和查询执行统计信息，解释性错误消息 OOM问题和Java堆转储分析，集群组件上的GC暂停，每个任务的独立日志给您很多有趣的时间 技术价格 每个节点几万到几十万美元 每个节点免费或高达数千美元 终端用户的可访问性 简单友好的SQL界面和简单可解释的数据库内功能 SQL并不完全符合ANSI，用户应该关心执行逻辑、底层数据布局。函数通常需要用Java编写、编译并放在集群上 目标最终用户受众 商业分析师 Java开发人员和经验丰富的DBA 单作业冗余 低，MPP节点失败时作业失败 High，只有当管理作业的节点执行作业时，作业才会失败 目标系统 通用DWH和分析系统 专用数据处理引擎 供应商锁定 典型案例 通常由技术滥用引起的罕见病例 最小推荐收藏大小 任何 千兆字节 最大并发 数以百计的查询 多达10-20的工作 技术延展性 仅使用供应商提供的工具 与任何全新的开源工具混合（Spark，Samza，Tachyon等） DBA技能水平要求 平均RDBMS DBA 一流的Java和RDBMS背景 解决方案实施复杂性 适度的 高 MPP架构和Batch架构结合总结","link":"/mpp-batch.html"},{"title":"Snowflake的标签功能","text":"今年 8 月份，Snowflake对外发布了对象标签功能的预览版，此功能通过应用业务上下文，例如将数据对象标识为敏感、PII 或属于成本中心的标签，使企业可以更轻松地了解和控制其数据。对象标记通过添加现有治理功能（例如 Snowflake 的动态数据屏蔽和行访问策略）来扩展 Snowflake 的本机数据治理功能。 了解数据所在的位置通常是通过适当的访问控制保护数据的第一步。对于敏感数据，例如带有财务报告的表或带有 PII 或 PHI 的列，了解数据在庞大数据资产中的位置对于满足法规遵从性要求至关重要。 对象标记的工作原理Snowflake 的对象标记功能提供了原生功能，可通过创建您自己的自定义标签库并将标签与所需对象（例如列、表/视图、数据库、仓库等）相关联来轻松解决这些用例。在将标签与对象相关联时，可以为标签分配字符串值。例如，您可以用 PII 标记一列，例如用 标记电话号码，用 标记PII = “Phone Number”包含电子邮件地址的另一列PII = “Email”。在本例中，PII是标签和”Phone Number”或是”Email”标签值。同样，您可以使用业务属性（例如成本中心或部门）标记仓库。例如，您可以标记一个专供销售部门使用的Department = “Sales”仓库和另一个专供财务部门使用的仓库Department = “Finance”. 如您所见，您可以根据部门细分您的信用消耗和使用情况以进行报告。 示例1：标记敏感数据让我们以管理员 Morgan 为例来看看使用对象标记功能的好处，他的任务是跟踪包含机密数据的表和带有 PII 的列。在此示例中，Morgan 创建了一个集中式标签库以实现跨数据资产的一致性，但依赖于数据所有者（例如 Alex）提供有关他们拥有的数据敏感性的输入。 Morgan 首先在名为 TAG_LIBRARY 的集中模式中创建一个标签库。这使得维护可应用于跨账户的账户和模式级对象的标签的集中分类变得更容易。Morgan 授予 Alex 将标签应用于 Alex 拥有的对象的特权，如下所示。 Alex 拥有 Clients 表。Clients 表是一个机密表，有一个包含 PII 的 Phone 列。创建表后，Alex 立即执行其作为数据所有者的职责，并使用相应的标签标记表和列，如下所示。 由于 Alex 等数据所有者使用集中标签为敏感数据分配适当的标签，Morgan 可以定期执行 PII 数据报告，并对敏感数据应用屏蔽策略。Morgan 使用新的 TAG_REFERENCES 帐户使用视图来跟踪新标记的对象。Morgan 还使用 POLICY_REFERENCES 帐户使用视图来确保敏感列具有与其关联的相应屏蔽策略。 示例2：标记成本中心PII 列跟踪示例说明了对象标记的工作原理，但它可用于解决其他用例。了解消耗数据或仓库信用的成本中心对于分解消耗以进行成本报告和退款非常重要。Tag_References 帐户使用视图可与 Warehouse_Metering_History 一起使用，以报告每个成本中心标签的仓库信用消耗，如下例所示。 功能价值 Consistent Assignment with Replilcation 标签继承 Ease of Use 易用性 Tag Lineage 标签血缘 Sensitive Data Tracking 敏感数据追踪 Centralized or Decentralized Management 集中或分散管理 Resource Usage 资源使用统计 Demo视频： 结论上面 snowflake 列举了两个示例场景： 标记敏感数据：给一些PII的列或数据打上安全标识的标签，对于有这些标签的数据可以统一应用一些屏蔽策略 标记成本中心：给一些warehouse打上部门标识的标签，再结合系统自带的使用统计表，统计每个成本中心的费用 综合来看，依靠灵活的标签功能，经过扩展，可以实现很多业务场景。 参考： https://www.snowflake.com/blog/object-tagging-is-now-available-in-public-preview/ https://medium.com/snowflake/snowflake-data-governance-object-tagging-overview-3f9acf431a0 https://docs.snowflake.com/en/user-guide/object-tagging.html#label-tags-track-usage","link":"/snowflake-object-tagging.html"},{"title":"产品书籍摘要","text":"自从加入产品经理大军之后，买了一些产品经理的书籍，也通过微信阅读看了一些书，以下记录一些觉得对自己有所感悟、收获、灵感的片段，陆续更新中… ###《产品之旅 - 产品经理的方法论和实战进阶》 需求分析的 “ 3+1 思考法” 需求从哪里来，目标用户是谁？ 到底是我们想做，还是客户想要，抑或是老板想要，给谁解决问题。 有多少人有这样的需求？这个需求紧迫吗？ 有多人有这样的需求表名市场的容量，紧迫程度意味着解决需求的价值 用户的痛点是什么？使用场景是什么？ 解决了什么问题，不解决这个问题会存在什么问题，用户问题的使用场景我们是不是真的找到了 ( +1 ) 怎么验证需求是否解决与解决效果如何？ 解决这些需求之后看网站数据会有什么表现？是网站数据提升了，还是用户反馈效果比较好。 总结：Where/who -&gt; how many -&gt; Pain -&gt; effect 《谷歌和亚马逊如何做产品》 以客户未导向，而不是以竞争为导向 — 贝索斯 《人人都是产品经理》 管理的能力，其实就是“在资源不足的情况下把事情做成”的能力 分析需求的商业价值 一个公司做任何产品，一个产品做任何需求，最终都是要满足一定的商业目的，所以”需求的商业价值”是最关键的内容，有条件的团队最好利用群体智慧，我们通常这个时候举行“需求讨论会” 衡量需求的商业价值，主要有以下几个维度：性价比 = 商业价值 / 实现难度(简化为开发量) 需求属性 属性说明 重要性 重要程度 紧急度 紧急程度 持续时间 持续时间 商业价值(*) 商业优先级，不考虑实现难度，群体决策","link":"/pm-book-summary.html"},{"title":"有效的管理邮件","text":"邮件，作为工作中很重要的东西。工作多年，有自己使用的一些方法和技巧。最近也参加了一个知乎关于邮件处理的live，总结下关于邮件处理的应该注意的一些心得和技巧，合理的利用这些在短期内提高管理电子邮件的效率。 Zero InboxZero Inbox，意为零收件箱。对每封邮件做到当天处理，保持收件箱清空状态。 将立即回复和定时回复结合起来，对重要联系人（客户、领导）发来的邮件，或者包含重要事情的邮件，迅速采取相应的行动。其他邮件，可以在一天中选择固定的时间进行统一处理。 在结束一天工作时，让收件箱空空如也，保证所有的事情都被处理妥当，这不是一个容易养成的习惯。却是可以带来很大的收获，让每天的工作更严谨效率。 用好过滤器设置邮件规则，在收到或发送一封邮件时，能够指挥邮箱对每封邮件进行一系列的条件判断，再根据预设的动作，对邮件进行处理。自定义常用的规则，达到自己满意的效果。 过滤器常用的条件包含： 发件人和收件人是谁 主题或正文包含什么关键字 过滤器预设的动作包含： 转移、自动回复、删除 标记已读 打标签 对每封邮件进行4D处理4D指，行动(Do)，转发(Delegate)，搁置(Defer)，删除（Delete） 行动 当发现邮件内容是由你来完成，且能在短时间内完成，则可以立即采取行动。 转发 当发现邮件有更适合的人去完成，或更低成本的完成的时候，可以选择转发。 搁置 当前有重要事情在做，且该邮件内容不能短时间完成，则可以放入到搁置文件夹中。 删除 可以把通知类邮件，放入 已处理 或 通知 文件夹。 写邮件技巧掌握并是用这些技巧，真正言简意赅、目的明确的表达邮件内容，写出高质量的邮件，让收件人处理邮件更为轻松。 邮件模版 配置常用的邮件模版，比如签名档 善于利用标题 增加&lt;信息&gt; &lt;请求&gt; &lt;行动&gt; &lt;周报&gt; &lt;日报&gt; &lt;培训&gt; &lt;重要&gt;等标注，能突出内容主题 长邮件提醒 对于长邮件，在邮件开头做说明，并提取重要信息阐述 长邮件结构清晰 利用标题、颜色、加粗、序号等方式，清晰表达邮件内容 给收件人多种选择 如果需要收件人采取行动时，可以提供多种选择，避免邮件长时间没得到结果 重读邮件 每次发送前，重复读一下邮件内容，语句通顺，主题明确，逻辑清晰 总结行动要点 如果需要收件人采取行动，告知收件人行动要点。 一封邮件一个主题 尽量一封邮件只说一件事情，或者汇总相关的事情。 写好一封邮件ABC法则，是一个邮件的框架，帮助你把信息放在正确的位置，把内容分成三个部分，简述Action、背景Backgroud、结尾Close 简述Action 简明的阐述行动的目的和关键点、点明前因后果，做到准确明白 背景Backgroud 清楚、简洁、相关 结尾Close 阐述下一步行动和小提示、签名 示例： 写邮件跟写博客文章一样，需要经常总结和应用合适的方法和技巧，不日之后，即可顺手拈来。 本文内容多为网上关于邮件管理的经验之谈，这里做一个适合自己的邮件管理的汇总笔记。","link":"/effectively-mail.html"},{"title":"构建大数据平台监控体系","text":"1. 监控体系概述 监控体系是平台的后盾，系统异常可能造成巨大的损失，有后盾才能放心冲锋，完善监控是运维人的重要工作之一。 监控系统建设应该以结果为导向，告警宜准不宜多；提供告警分析辅助工具，减少定位时间；监而不控，等于没有监控。 监控不仅是维稳，还应该引导提升业务体验，性能监控对持续引导优化业务体验十分重要，要找准核心指标，并提供辅助指标，核心指标要切中要害，覆盖面要广。 2. 监控体系定义 发现问题：当系统发生故障报警，我们会收到故障报警的信息 定位问题：故障邮件需要描述具体故障的内容，我们需要对报警内容进行分析故障具体原因。 解决问题：当然我们了解到故障的原因后，就需要通过故障解决的优先级去解决该故障。 总结问题：当我们解决完重大故障后，需要对故障原因以及防范进行总结归纳，避免以后重复出现。 3. 监控指标及措施3.1 硬件Hadoop集群和元数据库中的节点都需要监控。 功能分类 监控指标 优先级 网络 延时 500ms以内丢包率 高 磁盘 存储占用率 80%或不足20G读写IO 高 内存 占用率 90% 高 CPU 进程数 1024以内 高 监控措施： 监控人：集群运维部门，同时需要通知到Kyligence平台负责人 监控方式：客户需要提供监控措施，使用工具对环境中的硬件信息进行监控，并提供短信或邮件告警。 异常处理： 问题影响：环境不稳定，可能会导致集群不稳定或不可用。 问题处理：遇到问题，需要及时协调人员排查原因及解决 3.2 软件3.2.1 Hadoop如果是读写分离模式，包含构建和查询Hadoop集群。 功能分类 监控指标 优先级 HDFS 存储占用率 90%HDFS读写速度 是否占用集群吞吐60% 高 Hive Hive服务状态 高 Zookeeper Zookeeper服务状态 高 Yarn 能正常提交和执行任务占用资源多的构建任务 占队列50% 高 Hadoop节点 节点状态，是否正常 高 监控措施： 监控人：集群运维部门，同时需要通知到平台负责人 监控方式：使用工具对环境中的硬件信息进行监控，并提供短信或邮件告警。 3.2.2 元数据库 功能分类 监控指标 MySQL 服务架构，是否合理 服务状态，是否可用 高 性能指标 , 读写操作延时不超过1000ms 高 连接数，是否超1000 中 监控人：整理MySQL服务架构方案，确保高可用且负载均衡，配置符合产品要求规范，集群运维部门，同时需要通知到平台负责人 监控方式：客户需要提供监控措施，使用工具对环境中的硬件信息进行监控，并提供短信或邮件告警。 问题影响：MySQL环境不稳定，可能会导致集群不稳定或不可用。 问题处理：遇到问题，需要及时协调人员排查原因，确定影响及及时解决","link":"/ali-dev-manual.html"},{"title":"阿里开发手册摘要","text":"阿里作为全世界Java开发规模最大和应用水平最高的地方之一，三十几页的规范，从编程、异常日志、SQL、工程、安全五块形成的规约总结，根据约束力，分强制、推荐、参考三大类，可以作为优秀技术团队的重要利器。 官方文档地址：https://yq.aliyun.com/articles/69327?utm_content=m_10088 以下是阅读阿里开发手册中，记录的一些重要摘要信息。 编程规范命名 尽量语义清晰、望名知义 遵从驼峰形式 如果使用到了设计模式，类名体现具体模式 常量定义 值定义不要出现未定义的常量 常量类推荐也分类定义常量类，不要大而全的一个常量类 变量值在一定范围，使用Enum类 格式规约 单行限制不超过120个字符 看注释内容，示例代码： 123456789101112131415161718public static void main(String[] args) { // 缩进4个空格 String say = &quot;hello&quot;; // 运算符的左右必须有一个空格 int flag = 0; // 关键词if与括号之间必须有一个空格，括号内的f与左括号，0与右括号不需要空格 if (flag == 0) { System.out.println(say); } // 左大括号前加空格且不换行；左大括号后换行 if (flag == 1) { System.out.println(&quot;world&quot;); // 右大括号前换行，右大括号后有else，不用换行 } else { System.out.println(&quot;ok&quot;); // 在右大括号后直接结束，则必须换行 }} OOP规约 overwrite方法，必须有@overwrite注解 静态方法和变量，直接通过类名访问，禁止对象访问，增加无谓的编译器解释成本 不能使用过时的类或方法 包装类之间的值比较，使用equals方法 构造方法禁止加入任何业务逻辑，请放在init方法中 String的split方法，做分隔符后无内容的检测 类方法定义的顺序，公有方法或保护方法 &gt; 私有方法 &gt; getter/setter方法 getter/setter方法尽量不要加入任何业务逻辑，增加排查问题的难度 推荐使用StringBuilder的append方法替换string拼接 集合处理并发处理 线程定义名称 使用线程池，使用ThreadPoolExecutor 高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁；能锁区块，就不要锁整个方法体；能用对象锁，就不要用类锁。 使用CountDownLatch进行异步转同步操作，每个线程退出前必须调用countDown方法，线程执行代码注意catch异常，确保countDown方法可以执行，避免主线程无法执行至await方法，直到超时才返回结果。 控制语句1.推荐尽量少用else， if-else的方式可以改写成： 12345if (condition) { ... return obj;}// 接着写else的业务逻辑代码; a 注释 类、属性、方法，使用/*内容/定义注释，不得使用//注释 抽象方法、枚举，详细注释 注释掉的代码，配合说明 异常日志异常处理 RuntimeException预先检查进行规避，避免IndexOutofBoundsException/NullPointerException 异常不能用来做流程控制、条件控制，比条件判断效率低 finally需要关闭资源、流、连接等 避免出现重复代码 日志 使用SLF4J日志框架的API，不直接使用log4j/logback 日志文件命名，含义清晰 异常信息，包含现场信息和堆栈信息1logger.error(各类参数或者对象toString + &quot;_&quot; + e.getMessage(), e); 使用条件输出或占位输出123456正例：（条件）if (logger.isDebugEnabled()) { logger.debug(&quot;Processing trade with id: &quot; + id + &quot; symbol: &quot; + symbol);}正例：（占位符）logger.debug(&quot;Processing trade with id: {} symbol : {} &quot;, id, symbol); MySQLMySQL规约中，MySQL使用方法都是比较有用的经验之选，推荐都仔细研读。 建表 使用小写字母，数字，下划线命名 尽量不修改表名、字段名 主键pk_ ,索引 idx_ ,唯一 uk_ 小数类型为decimal，禁止使用float和double。 如果存储的字符串长度几乎相等，使用char定长字符串类型 索引 join字段类型一致、禁止超过3表join 在varchar字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度即可。 说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为20的索引，区分度会高达90%以上，可以使用count(distinct left(列名, 索引长度))/count(*)的区分度来确定。 页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。右模糊可以利用b+树最左前缀匹配 利用延迟关联或者子查询优化超多分页场景。 说明：MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行SQL改写。 正例：先快速定位需要获取的id段，然后再关联： SELECT a.* FROM 表1 a, (select id from 表1 where 条件 LIMIT 100000,20 ) b where a.id=b.id SQL规约 【强制】不要使用count(列名)或count(常量)来替代count()，count()是SQL92定义的标准统计行数的语法，跟数据库无关，跟NULL和非NULL无关。 说明：count(*)会统计值为NULL的行，而count(列名)不会统计此列为NULL值的行。 【强制】count(distinct col) 计算该列除NULL之外的不重复行数，注意 count(distinct col1, col2) 如果其中一列全为NULL，那么即使另一列有不同的值，也返回为0。 【强制】当某一列的值全是NULL时，count(col)的返回结果为0，但sum(col)的返回结果为NULL，因此使用sum()时需注意NPE问题。 ORM规约 在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。 @Transactional事务不要滥用。事务会影响数据库的QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。 工程规约应用分层 图中默认上层依赖于下层，箭头关系表示可直接依赖，如：开放接口层可以依赖于Web层，也可以直接依赖于Service层，依此类推： 开放接口层：可直接封装Service方法暴露成RPC接口；通过Web封装成http接口；进行网关安全控制、流量控制等。 终端显示层：各个端的模板渲染并执行显示的层。当前主要是velocity渲染，JS渲染，JSP渲染，移动端展示等。 Web层：主要是对访问控制进行转发，各类基本参数校验，或者不复用的业务简单处理等。 Service层：相对具体的业务逻辑服务层。 Manager层：通用业务处理层，它有如下特征： 1） 对第三方平台封装的层，预处理返回结果及转化异常信息； 2） 对Service层通用能力的下沉，如缓存方案、中间件通用处理； 3） 与DAO层交互，对多个DAO的组合复用。 DAO层：数据访问层，与底层MySQL、Oracle、Hbase进行数据交互。 外部接口或第三方平台：包括其它部门RPC开放接口，基础平台，其它公司的HTTP接口。 服务器规约 给JVM设置-XX:+HeapDumpOnOutOfMemoryError参数，让JVM碰到OOM场景时输出dump信息。 高并发服务器建议调小TCP协议的time_wait超时时间。 安全规约用户请求传入的任何参数必须做有效性验证。 说明：忽略参数校验可能导致： page size过大导致内存溢出 恶意order by导致数据库慢查询 任意重定向 SQL注入 反序列化注入 正则输入源串拒绝服务ReDoS 附录分层领域模型规约 DO（Data Object）：与数据库表结构一一对应，通过DAO层向上传输数据源对象。 DTO（Data Transfer Object）：数据传输对象，Service和Manager向外传输的对象。 BO（Business Object）：业务对象。可以由Service层输出的封装业务逻辑的对象。 QUERY：数据查询对象，各层接收上层的查询请求。注：超过2个参数的查询封装，禁止使用Map类来传输。 VO（View Object）：显示层对象，通常是Web向模板渲染引擎层传输的对象。 专有名词 POJO（Plain Ordinary Java Object）：在本规约中，POJO专指只有setter/getter/toString的简单类，包括DO/DTO/BO/VO等。 DO（Data Object）：本手册指数据库表一一对应的POJO类。 GAV（GroupId、ArtifactctId、Version）：Maven坐标，是用来唯一标识jar包。 OOP（Object Oriented Programming）: 本手册泛指类、对象的编程处理方式。 ORM（Object Relation Mapping）: 对象关系映射，对象领域模型与底层数据之间的转换，本文泛指iBATIS, mybatis等框架。 NPE（java.lang.NullPointerException）: 空指针异常。 一方库：本工程内部子项目模块依赖的库（jar包）。 二方库：公司内部发布到中央仓库，可供公司内部依赖的库（jar包）。 三方库：公司之外的开源的依赖库（jar包） 官方文档地址：https://yq.aliyun.com/articles/69327?utm_content=m_10088","link":"/ali-dev-manual.html"},{"title":"Nginx的五种负载均衡策略","text":"Nginx可以按照调度规则实现动态、静态页面的分离，也可以按照轮询、ip哈希、URL哈希、权重等多种方式对后端服务器做负载均衡，同时还支持后端服务器的健康检查。 nginx的upstream目前支持的5种方式的分配 1.轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 1234upstream backserver { server 192.168.0.14; server 192.168.0.15; } 2.指定权重 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 1234upstream backserver { server 192.168.0.14 weight=10; server 192.168.0.15 weight=10; } 3.IP绑定 ip_hash (session绑定) 每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 12345upstream backserver { ip_hash; server 192.168.0.14:88; server 192.168.0.15:80; } 4.fair（第三方） 按后端服务器的响应时间来分配请求，响应时间短的优先分配。 12345upstream backserver { server server1; server server2; fair; } 5.url_hash（第三方） 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 123456upstream backserver { server squid1:3128; server squid2:3128; hash $request_uri; hash_method crc32; } 在需要使用负载均衡的server中增加示例配置 123456789101112proxy_pass http://backserver/; upstream backserver{ ip_hash; server 127.0.0.1:9090 down; (down 表示单前的server暂时不参与负载) server 127.0.0.1:8080 weight=2; (weight 默认为1.weight越大，负载的权重就越大) server 127.0.0.1:6060; server 127.0.0.1:7070 backup; (其它所有的非backup机器down或者忙的时候，请求backup机器) } max_fails ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误fail_timeout:max_fails次失败后，暂停的时间","link":"/nginx-load-balancing.html"}],"tags":[{"name":"大数据","slug":"大数据","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"计算机","slug":"计算机","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MPP","slug":"MPP","link":"/tags/MPP/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"Snowflake","slug":"Snowflake","link":"/tags/Snowflake/"},{"name":"标签","slug":"标签","link":"/tags/%E6%A0%87%E7%AD%BE/"},{"name":"数据资产管理","slug":"数据资产管理","link":"/tags/%E6%95%B0%E6%8D%AE%E8%B5%84%E4%BA%A7%E7%AE%A1%E7%90%86/"},{"name":"产品经理","slug":"产品经理","link":"/tags/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/"},{"name":"摘要","slug":"摘要","link":"/tags/%E6%91%98%E8%A6%81/"},{"name":"邮件","slug":"邮件","link":"/tags/%E9%82%AE%E4%BB%B6/"},{"name":"管理","slug":"管理","link":"/tags/%E7%AE%A1%E7%90%86/"},{"name":"阿里","slug":"阿里","link":"/tags/%E9%98%BF%E9%87%8C/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"规范","slug":"规范","link":"/tags/%E8%A7%84%E8%8C%83/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"负载均衡","slug":"负载均衡","link":"/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"}],"categories":[{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"产品经理","slug":"产品经理","link":"/categories/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/"},{"name":"观点感悟","slug":"观点感悟","link":"/categories/%E8%A7%82%E7%82%B9%E6%84%9F%E6%82%9F/"},{"name":"编程技术","slug":"编程技术","link":"/categories/%E7%BC%96%E7%A8%8B%E6%8A%80%E6%9C%AF/"}]}